%%% Title:    DSS Stats & Methods: Lecture 3
%%% Author:   Kyle M. Lang
%%% Created:  2015-11-06
%%% Modified: 2020-08-26

\documentclass[10pt]{beamer}
\usetheme{Utrecht}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{eurosym}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{listings}
\usepackage{xspace}
\usepackage{tcolorbox}
\tcbuselibrary{listings}
\usepackage{hyperref}

\hypersetup{
  colorlinks = false,
  linkcolor = blue,
  filecolor = blue,
  citecolor = black,
  urlcolor = blue
}

\definecolor{codebackground}{RGB}{224,234,238}
\definecolor{codestring}{RGB}{191,3,3}
\definecolor{codekeyword}{RGB}{1,1,129}
\definecolor{codecomment}{RGB}{131,129,131}

\newcommand{\src}[1]{%
  \tcbox[%
  on line,
  colback = codebackground,
  colframe = codebackground,
  left = 0pt,
  right = 0pt,
  top = 0pt,
  bottom = 0pt%
  ]{%
    \lstinline[%
    language = R,
    basicstyle = \ttfamily,
    keywordstyle = \color{codekeyword},
    commentstyle = \color{codecomment}\itshape,
    stringstyle = \color{codestring},
    deletekeywords = {_}
    % frame = single,
    % frameround = tttt,
    % fillcolor = \color{blue}%
    ]{#1}%
  }
}

%% The following command was adapted from LaTeX Community user 'localghost':
\newcommand*\bigbar[1]{%
  \hbox{%
    \vbox{%
      \hrule height 0.65pt % The actual bar
      \kern0.35ex%         % Distance between bar and symbol
      \hbox{%
        \kern-0.1em%      % Shortening on the left side
        \ensuremath{#1}%
        %\kern-0.1em%      % Shortening on the right side
      }%
    }%
  }%
} 

\newcommand{\rmsc}[1]{\textrm{\textsc{#1}}}
\newcommand{\pkg}[1]{\textbf{#1}}
% \newcommand{\src}[1]{\texttt{#1}}

\newcommand{\pipe}{\texttt{\%>\%}}
\newcommand{\expipe}{\texttt{\%\$\%}}
\newcommand{\apipe}{\texttt{\%<>\%}}
\newcommand{\rpipe}{\texttt{|>}}

\title{Data Cleaning, Data Visualization, \& Functions}
\subtitle{Fundamental Techniques in Data Science}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Utrecht University}
\date{}

<<setup, include = FALSE, cache = FALSE>>=
set.seed(235711)

library(knitr)
library(dplyr)
library(magrittr)
library(xtable)

                                        #source("../../../code/supportFunctions.R")

options(width = 60)
opts_chunk$set(size = "footnotesize",
               fig.align = "center",
               fig.path = "figure/r_basics-",
               message = FALSE,
               warning = FALSE,
               comment = "")
knit_theme$set('edit-kwrite')
@

\begin{document}
  
%------------------------------------------------------------------------------%

\begin{frame}[t,plain]
  \titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%------------------------------------------------------------------------------%

\sectionslide{Data Analytic Lifecycle}

%------------------------------------------------------------------------------%

\begin{frame}{Research Cycle}

  The following is a representation of the \emph{Research Cycle} used for 
  empirical research in most of the sciences.
  
  \begin{figure}
    \includegraphics[width = 0.7\textwidth]{figures/research_cycle.pdf}
  \end{figure}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{CRISP-DM}

  The \emph{Cross-industry Standard Process for Data Mining} was developed to 
  standardized the process of data mining in industry applications.
  
  \begin{figure}
    \includegraphics[width = 0.7\textwidth]{figures/crisp-dm.pdf}
  \end{figure}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Data Science Cycle}
  
  The \emph{Data Science Cycle} represented here was adapted from 
  \citet{o'neilSchutt:2014}.
  
  \begin{figure}
    \includegraphics[width = 0.7\textwidth]{figures/data_science_cycle.pdf}
  \end{figure}
  
\end{frame}

%------------------------------------------------------------------------------%

\sectionslide{Data Cleaning}

%------------------------------------------------------------------------------%

\begin{frame}{Data Cleaning}
  
  When we receive new data, they are generally messy and contaminated by various 
  anomalies and errors.
  \vb
  \begin{itemize}
  \item One of the first steps in processing a new set of data is 
    \emph{cleaning}.
    \vc
  \item By cleaning the data, we ensure a few properties:
    \vc
    \begin{itemize}
    \item The data are in an analyzable format.
      \vc
    \item All data take legal values.
      \vc
    \item Any outliers are located and treated.
      \vc
    \item Any missing data are located and treated.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\subsection{Missing Data}

%------------------------------------------------------------------------------%

\begin{frame}{A Little Notation}
  
  \begin{align*}
    Y &\coloneqq \text{An $N \times P$ data matrix}\\
    \\
    Y_{mis} &\coloneqq \text{The \emph{missing} part of $Y$}\\
    \\
    Y_{obs} &\coloneqq \text{The \emph{observed} part of $Y$}\\
    \\
    R &\coloneqq \text{An $N \times P$ pattern matrix encoding nonresponse}
  \end{align*}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{What are Missing Data?}
  
  Missing data are empty cells in a dataset where there should be observed 
  values.
  \vc
  \begin{itemize}
  \item The missing cells correspond to true population values, but we haven't 
    observed those values.
  \end{itemize}
  \vb 
  \pause
  Not every empty cell is a missing datum.
  \vc
  \begin{itemize}
  \item Quality-of-life ratings for dead patients in a mortality study
    \vc
  \item Firm profitability after the company goes out of business
    \vc
  \item Self-reported severity of menstrual cramping for men
    \vc
  \item Empty blocks of data following ``gateway'' items
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%
    
\begin{frame}
  
  \begin{center}
    \huge{Missing Data Descriptives}
  \end{center}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\captionsetup{labelformat = empty}

\begin{frame}{Missing Data Pattern}
  
<<echo = FALSE>>=
tmpTab <- matrix(c("x", "x", ".", ".",
                   "y", ".", "y", "."),
                 ncol = 2,
                 dimnames = list(NULL, c("X", "Y"))
                 )

patTab1 <- xtable(tmpTab, align = rep("c", 3), caption = "Patterns for $P = 2$")

tmpTab <- matrix(c(rep("x", 3), ".", "x", rep(".", 3),
                   "y", "y", ".", "y", ".", ".", "y", ".",
                   "z", ".", "z", "z", ".", "z", ".", "."),
                 ncol = 3,
                 dimnames = list(NULL, c("X", "Y", "Z"))
                 )

patTab2 <- xtable(tmpTab, align = rep("c", 4), caption = "Patterns for $P = 3$")
@ 

Missing data (or response) patterns represent unique combinations of observed
and missing items.
\begin{itemize}
  \item $P$ items $\Rightarrow$ $2^P$ possible patterns.
\end{itemize}

\begin{columns}
  \begin{column}{0.45\textwidth}
    
<<echo = FALSE, results = 'asis'>>=
print(patTab1, booktabs = TRUE)
@ 
    
  \end{column}
  \begin{column}{0.45\textwidth}
\vx{-12}    
<<echo = FALSE, results = 'asis'>>=
print(patTab2, booktabs = TRUE)
@ 
     
    \end{column}
  \end{columns}
  
\end{frame}

\captionsetup{labelformat = default}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Nonresponse Rates}
  
  \textsc{Percent/Proportion Missing}
  \begin{itemize}
  \item The proportion of cells containing missing data
  \item Good early screening measure
  \item Should be computed for each variable, not for the entire dataset
  \end{itemize}
  
  \va
  
  \textsc{Attrition Rate}
  \begin{itemize}
  \item The proportion of participants that drop-out of a study at each 
    measurement occasion
  \end{itemize}
  
  \va
  
  \textsc{Percent/Proportion of Complete Cases}
  \begin{itemize}
  \item The proportion of observations with no missing data
  \item Often reported but nearly useless quantity
  \end{itemize}
  
  \pagebreak
  
  \textsc{Covariance Coverage}
  \begin{itemize}
  \item The proportion of cases available to estimate a given pairwise
    relationship (e.g., a covariance between two variables)
  \item Very important to have adequate coverage for the parameters you want to 
    estimate
  \end{itemize}
  
  \va
  
  \textsc{Fraction of Missing Information}
  \begin{itemize}
  \item Associated with an estimated parameter, not with an incomplete variable
  \item Like an $R^2$ for the missing data
  \item Most important diagnostic value for missing data problems
  \item Can only be computed after treating the missing data
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Covariance Coverage Examples}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      \begin{itemize}
      \item What is the coverage for $cov(X, Y)$?
        \vc
      \item What is the coverage for $cov(W, Y)$?
        \vc
      \item What about $cov(X, Z)$?
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, results = 'asis'>>=
tmpTab4 <- matrix(c(rep("w", 10), 
                    rep("x", 5), rep(".", 5),
                    rep("y", 10),
                    rep(".", 5), rep("z", 5)), 
                 ncol = 4)
colnames(tmpTab4) <- c("W", "X", "Y", "Z")

patTab4 <- xtable(tmpTab4, align = rep("c", 5))
print(patTab4, booktabs = TRUE)
@

\end{column}
\end{columns}
 
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Nonresponse Rate Examples}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      \begin{itemize}
        \item What is the percent missing at Time 2?
          \vc
        \item What is the attrition rate at Time 3?
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, results = 'asis'>>=
tmpTab5 <- matrix(c(rep("x1", 10), 
                    rep("x2", 7), rep(".", 3),
                    rep("x3", 5), rep(".", 5),
                    rep("x4", 3), rep(".", 7)), 
                 ncol = 4)
colnames(tmpTab5) <- c("T1", "T2", "T3", "T4")

patTab5 <- xtable(tmpTab5, align = rep("c", 5))
print(patTab5, booktabs = TRUE)
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\subsection{Outliers}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{What is an outlier?}
  
  For the time being, we're considering \emph{univariate outliers}.
  \vb
  \begin{itemize}
  \item Extreme values with respect to the distribution of a variable's other 
    observations
    \vc
    \begin{itemize}
    \item A human height measurement of 3 meters
      \vc
    \item A high temperature in Tilburg of $50^\circ$
      \vc
    \item Annual income of \euro250,000 for a student
    \end{itemize}
    \vb
  \item Not accounting for any particular model (we'll get to that later)
  \end{itemize}
  
  \pagebreak
  
  A univariate outlier may, or may not, be an illegal value.
  \vb
  \begin{itemize}
  \item Data entry errors are probably the most common cause.
    \vc
  \item Outliers can also be legal, but extreme, values.
  \end{itemize}
  
  \va
  
  \textsc{Key Point:} We choose to view an outlier as arising from a different 
  population than the one to which we want to generalize our findings.
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Finding Univariate Outliers}

  We have many methods available to diagnose potential outliers.
  \vb
  \begin{itemize}
  \item Four of the simplest and most popular are:
    \vc
    \begin{enumerate}
    \item Internally studentized residuals (AKA Z-score method)
      \vc
    \item Externally studentized residuals
      \vc
    \item Median absolute deviation method
      \vc
    \item Tukey's boxplot method
    \end{enumerate}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Internally Studentized Residuals}

  For each observation, $X_n$, we compute the following quantity:
  \begin{align*}
    T_n = \frac{X_n - \bigbar{X}}{SD_X}
  \end{align*}
  \vx{-12}
  \begin{itemize}
  \item $T_n$ follows a Student's $t$ distribution with $df = N - 1$.
    \begin{itemize}
    \item We can do a formal test for ``outlier'' status.
    \end{itemize}
  \item Assuming a large sample, if $T_n > C$ (where $C$ is usually 2 or 3), we 
    label $X_n$ as an outlier.
  \end{itemize}
  
  \vb
  \pause
  
  Although simple, this method has some substantial limitations.
  \begin{itemize}
  \item The cutpoint, $C$, can only be meaningfully chosen when $X$ is normally 
    distributed.
  \item Both $\bigbar{X}$ and $SD_X$ are highly sensitive to outliers.
  \end{itemize}
  
\end{frame}


%------------------------------------------------------------------------------%

\begin{frame}{Externally Studentized Residual}

  The externally studentized residual method is essentially the same as the 
  internally studentized residual method, but we adjust $\bigbar{X}$ and $SD_X$ 
  to remove the influence of the observation we're evaluating.
  \vb
  \begin{itemize}
  \item Let $\mathbb{N}_{(n)} = \{1, \ldots, (n - 1), (n + 1), \ldots, N\}$. 
  \item Define the deletion mean, $\bigbar{X}_{(n)}$, and deletion SD, 
    $SD_{X(n)}$, as:
    \begin{align*}
      \bigbar{X}_{(n)} &= \frac{1}{N - 1} \sum_{i \in \mathbb{N}_{(n)}} X_i\\
      SD_{X(n)} &= \sqrt{\frac{1}{N - 2} \sum_{i \in \mathbb{N}_{(n)}} \left(X_i - \bigbar{X}_{(n)}\right)^2}
    \end{align*}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Externally Studentized Residual}
  
  The externally studentized residual is defined in the same way as the 
  internally studentized version:
  \begin{align*}
    T_{(n)} = \frac{X_n - \bigbar{X}_{(n)}}{SD_{X(n)}}
  \end{align*}
  \vx{-12}
  \begin{itemize}
  \item $T_{(n)}$ follows a Student's $t$ distribution with $df = N - 2$.
    \begin{itemize}
    \item We can do a formal test for ``outlier'' status.
    \end{itemize}
  \item Assuming a large sample, if $T_{(n)} > C$ (where $C$ is usually 2 or 3), 
    we label $X_n$ as an outlier.
  \end{itemize}
  
  \vb
  \pause
  
  $T_{(n)}$ is immune to the influence of the $n$th observation.
  \begin{itemize}
  \item Still requires $X$ to be normally distributed
  \item Still sensitive to outliers other than the $n$th observation
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Median Absolute Deviation Method}
  
<<echo = FALSE>>=
q <- qnorm(0.75)
b <- 1 / q
@ 

The biggest limitation of studentized residuals is that their measures of 
central tendency and dispersion are sensitive to outliers.
\vb
\begin{itemize}
\item If we can replace the (deletion) mean and the (deletion) SD with more 
  robust statistics, we can avoid this issue.
  \vb
  \begin{itemize}
  \item Replace the mean, $\bar{X}$, with the \emph{median}, $\text{Med}(X)$
    \vb
  \item Replace the SD with the \emph{median absolute deviation}:
    \begin{align*}
      MAD_X = b \times \text{Med} \left( \big| X_n - \text{Med} (X) \big| 
      \right)
    \end{align*}
  \item We choose the coefficient as $b = 1/Q_{0.75}$
    \vb
  \item For the normal distribution, $b \approx 1/\Sexpr{round(q, 4)} \approx 
    \Sexpr{round(b, 4)}$
  \end{itemize}
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Median Absolute Deviation Method}
  
  We compute our test statistic by replacing the mean with the median and the SD 
  with the MAD in the standard Wald test formula:
  \begin{align*}
    T_{MAD} = \frac{X_n - \text{Med}(X)}{MAD_X}
  \end{align*}
  \vx{-12}
  \begin{itemize}
  \item $T_{MAD}$ doesn't allow for formal statistical tests.
  \item We can use the same general cutoffs we would use for the studentized 
    residual methods.
    \begin{itemize}
    \item Assuming a large sample, if $T_{(n)} > C$ (where $C$ is usually 2 or 
      3), we label $X_n$ as an outlier.
    \end{itemize}
  \end{itemize}
  
  \vb
  \pause
  
  $T_{MAD}$ is immune to the influence of, up to, 50\% outlying observations.
  \begin{itemize}
  \item Requires us to assume a parametric distribution for $X$
    \begin{itemize}
    \item This assumption is necessary to compute $b$.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Breakdown Point}
  
  To compare robust statistics, we consider their \emph{breakdown points}.
  \begin{itemize}
  \item The breakdown point is the minimum proportion of cases that must be 
    replaced by $\infty$ to cause the value of the statistic to go to $\infty$.
  \end{itemize}
  \vc
  The mean has a breakdown point of $1 / N$.
  \begin{itemize}
  \item Replacing a single value with $\infty$ will produce an infinite mean.
  \end{itemize}
  \vc
  The deletion mean has a breakdown point of $2 / N$.
  \begin{itemize}
  \item We can replace, at most, 1 value with $\infty$ without producing an 
    infinite mean.
  \end{itemize}
  \vc
  The median has breakdown point of 50\%.
  \begin{itemize}
    \item We can replace $n < N / 2$ of the observations with $\infty$ without 
      producing an infinite median.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Boxplot Method}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      \citet{tukey:1977} described a procedure for flagging potential outliers 
      based on the familiar box-and-whiskers plot.
      \begin{itemize}
      \item Does not require normally distributed $X$
      \item Not sensitive to outliers
      \item Doesn't allow for formal statistical tests
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
data(Salaries, package = "carData")

boxplot(salary ~ sex,
        data = Salaries,
        ylab = "Salary",
        main = "9-Month Salaries of Professors")
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Boxplot Method}
  
  A \emph{fence} is an interval defined as the following function of the 
  \emph{first quartile}, the \emph{third quartile}, and the \emph{inner quartile
    range} ($IQR = Q_3 - Q_1$):
  \begin{align*}
    F = \{Q_1 - C \times IQR, Q_3 + C \times IQR\}
  \end{align*}
  
  \vx{-6}
  
  \begin{itemize}
  \item Taking $C = 1.5$ produces the \emph{inner fence}.
  \item Taking $C = 3.0$ produces the \emph{outer fence}.
  \end{itemize}
  
  \vb
  
  We can use these fences to identify potential outliers:
  \begin{itemize}
  \item Any value that falls outside of the inner fence is a \emph{possible 
    outlier}.
  \item Any value that falls outside of the outer fence is a \emph{probable 
    outlier}.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Multivariate Outliers}
  
  Sometimes, the combinations of values in an observation are very unlikely, 
  even when no individual value is an outlier.
  \vc
  \begin{itemize}
  \item These observations are \emph{multivariate outliers}.
    \vc
    \begin{itemize}
    \item A person in the 95\emph{th} percentile for height and the 5\emph{th} 
      percentile for weight
      \vc
    \item A person who simultaneously scores highly on scales of depression and 
      positive affect
    \end{itemize}
  \end{itemize}
  \vb
  To detect multivariate outliers, we use \emph{distance metrics}.
  \vc
  \begin{itemize}
  \item Distance metrics quantify the similarity of two vectors.
    \vc
    \begin{itemize}
    \item Similarity between two observations
      \vc
    \item Similarity between an observation and the mean vector
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Mahalanobis Distance}
  
  One of the most common distance metrics is the \emph{Mahalanobis Distance}.
  \vc
  \begin{itemize}
  \item The Mahalanobis distance, $\Delta$, is a multivariate generalization of 
    the internally studentized residual:
  \end{itemize}
  \begin{align*}
    \Delta_n = \sqrt{\left( \mathbf{x}_n - \hat{\mu}_{\mathbf{X}} \right) 
      \hat{\Sigma}_{\mathbf{X}}^{-1} \left( \mathbf{x}_n - \hat{\mu}_{\mathbf{X}} 
      \right)^T}
  \end{align*}
  As with studentized residuals, if $\Delta_n > C$, we label $\mathbf{x}_n$ as 
  an outlier.
  \vc
  \begin{itemize}
  \item When $\mathbf{X}$ is $K$-variate normally distributed, $\Delta_n^2$ 
    follows a $\chi^2$ distribution with $df = K$.
    \vc
  \item We take $C$ to be the square-root of a suitably conservative quantile 
    (e.g., $q \in \{99\%, 99.9\%\}$) of the $\chi_K^2$ distribution: 
    $C = \sqrt{\chi_{K, q}^2}$.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Problems with Mahalanobis Distance}
  
  Like the internally studentized residual, Mahalanobis distance is highly 
  sensitive to outliers.
  \vc
  \begin{itemize}
  \item The underlying estimates of central tendency, $\hat{\mu}_{\mathbf{X}}$, 
    and dispersion, $\hat{\Sigma}_{\mathbf{X}}$, are computed using all 
    observations.
  \end{itemize}
  \vb
  \pause
  We want robust analogues of $\hat{\mu}_{\mathbf{X}}$ and 
  $\hat{\Sigma}_{\mathbf{X}}$.
  \vc
  \begin{itemize}
  \item We have several options for robust estimation of $\hat{\mu}_{\mathbf{X}}$ 
    and $\hat{\Sigma}_{\mathbf{X}}$. E.g.:
    \begin{itemize}
    \item Minimum covariance determinant method \citep[MCD; ][]{rousseeuw:1985}
      \vc
    \item Minimum volume ellipsoid method \citep[MVE; ][]{rousseeuw:1985}
      \vc
    \item M-estimation \citep{maronna:1976}
    \end{itemize}
    \vc
  \item Conceptually, robust methods operate by either:
    \begin{itemize}
    \item Using only a ``good'' subset of data to estimate $\hat{\mu}_{\mathbf{X}}$ 
      and $\hat{\Sigma}_{\mathbf{X}}$.
      \vc
    \item Downweighting outlying observations when estimating 
      $\hat{\mu}_{\mathbf{X}}$ and $\hat{\Sigma}_{\mathbf{X}}$.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Robust Mahalanobis Distance}
  
  Equipped with robust estimates of central tendency, $\hat{\mu}_{R,\mathbf{X}}$, 
  and dispersion, $\hat{\Sigma}_{R,\mathbf{X}}$, we define the robust Mahalanobis 
  distance in the natural way:
  \begin{align*}
    \Delta_{R,n} = \sqrt{\left( \mathbf{x}_n - \hat{\mu}_{R,\mathbf{X}} \right) 
      \hat{\Sigma}_{R,\mathbf{X}}^{-1} \left( \mathbf{x}_n - \hat{\mu}_{R,\mathbf{X}} 
      \right)^T}
  \end{align*}
  We use $\Delta_{R,n}$ in the same way as $\Delta_n$.
  \vc
  \begin{itemize}
  \item If $\Delta_{R,n} > C$, we label $\mathbf{x}_n$ as an outlier.
    \vc
  \item Again, we take $C$ to be the square-root of some quantile of the 
    $\chi_K^2$ distribution: $C = \sqrt{\chi_{K, q}^2}$.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Practicalities: Univariate vs. Multivariate}

  Univariate outlier checks are safe for most variables.\\ 
  \vb
  \pause
  \textsc{\underline{Don't}} include too many variables in multivariate outlier 
  checks.
  \begin{itemize}
  \item More variables increases the chances of false positives.
  \item E.g., don't run a multivariate outlier test on your entire dataset.
  \end{itemize}
  \vb
  \pause
  \textsc{\underline{Do}} use multivariate outlier checks for scales.
  \begin{itemize}
  \item E.g., if you have a psychometric scale measuring depression, you should
    check the items of that scale for multivariate outliers.
  \end{itemize}
  \vb 
  \pause
  \textsc{\underline{Maybe}} check the variables in a single model for
  multivariate outliers.
  \begin{itemize}
  \item E.g., if you have a small set of items that you will include in a
    regression model, it could make sense to check these variables for
    multivariate outliers.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Practicalities: Outliers for Categorical Data}

  Nominal, ordinal, and binary items \emph{can} have outliers.
  \begin{itemize}
  \item Outliers on categorical variables are often more indicative of bad
    variables than outlying cases.
  \end{itemize}
  \vb
  \pause
  Ordinal
  \begin{itemize}
  \item Most participant endorse one of the lowest categories on an ordinal
    item, but a few participants endorse the highest category.
  \item The participants who endorse the highest category may be outliers.
  \end{itemize}
  \vb
  \pause
  Nominal
  \begin{itemize}
  \item Groups with very low membership may be outliers on nominal grouping
    variables.
  \end{itemize}
  \vb
  \pause
  Binary
  \begin{itemize}
  \item If most endorse the item, the few who do not may be outliers.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Treating Outliers}
  
  If we locate any outliers, they must be treated.
  \vc
  \begin{itemize}
  \item Outliers cause by errors, mistakes, or malfunctions (i.e., \emph{error 
    outliers}) should be directly corrected.
    \vc
  \item Labeling non-error outliers is a subjective task.
    \begin{itemize}
    \item A (non-error) outlier must originate from a population separate from 
      the one we care about.
    \item Don't blindly automate the decision process.
    \end{itemize}
  \end{itemize}
  
  \pause
  \vb
 
  The most direct solution is to delete any outlying observation.
  \vc
  \begin{itemize}
  \item If you delete non-error outliers, the analysis should be reported twice: 
    with outliers and without.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Treating Outliers}
  
  For univariate outliers, we can use less extreme types of deletion.
  \begin{itemize}
  \item Delete outlying values (but not the entire observation).
  \item These empty cells then become missing data.
  \end{itemize}
  \vb
  Winsorization:
  \begin{itemize}
  \item Replace the missing values with the nearest non-outlying value.
  \end{itemize}
  \vb
  Missing data analysis:
  \begin{itemize}
  \item Treat the missing values along with any naturally-occurring nonresponse.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Treating Outliers}
  
  We can also use robust regression procedures to estimate the model directly in 
  the presence of outliers.
  \vc
  \begin{itemize}
  \item Weight the objective function to reduce the impact of outliers
    \begin{itemize}
    \item M-estimation
    \end{itemize}
    \vc
  \item Trim outlying observations during estimation 
    \begin{itemize}
    \item Least trimmed squares, MCD, MVE
    \end{itemize}
    \vc
  \item Take the median, instead of the mean, of the squared residuals
    \begin{itemize}
    \item Least median of squares
    \end{itemize}
    \vc
  \item Model some quantile of the DV's distribution instead of the mean 
    \begin{itemize}
    \item Quantile regression
    \end{itemize}
    \vc
  \item Model the outcome with a heavy-tailed distribution
    \begin{itemize}
    \item Laplacian, Student's T
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{References}

  \bibliographystyle{apacite}
  \bibliography{../../../bibtex/stat_meth_refs.bib}

\end{frame}

%------------------------------------------------------------------------------%

\end{document}
