%%% Title:    DSS Stats & Methods: Lecture 3
%%% Author:   Kyle M. Lang
%%% Created:  2015-11-06
%%% Modified: 2020-08-26

\documentclass[10pt]{beamer}
\usetheme{Utrecht}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{eurosym}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{listings}
\usepackage{xspace}
\usepackage{tcolorbox}
\tcbuselibrary{listings}
\usepackage{hyperref}

\hypersetup{
  colorlinks = false,
  linkcolor = blue,
  filecolor = blue,
  citecolor = black,
  urlcolor = blue
}

\definecolor{codebackground}{RGB}{224,234,238}
\definecolor{codestring}{RGB}{191,3,3}
\definecolor{codekeyword}{RGB}{1,1,129}
\definecolor{codecomment}{RGB}{131,129,131}

\newcommand{\src}[1]{%
  \tcbox[%
  on line,
  colback = codebackground,
  colframe = codebackground,
  left = 0pt,
  right = 0pt,
  top = 0pt,
  bottom = 0pt%
  ]{%
    \lstinline[%
    language = R,
    basicstyle = \ttfamily,
    keywordstyle = \color{codekeyword},
    commentstyle = \color{codecomment}\itshape,
    stringstyle = \color{codestring},
    deletekeywords = {_}
    % frame = single,
    % frameround = tttt,
    % fillcolor = \color{blue}%
    ]{#1}%
  }
}

%% The following command was adapted from LaTeX Community user 'localghost':
\newcommand*\bigbar[1]{%
  \hbox{%
    \vbox{%
      \hrule height 0.65pt % The actual bar
      \kern0.35ex%         % Distance between bar and symbol
      \hbox{%
        \kern-0.1em%      % Shortening on the left side
        \ensuremath{#1}%
        %\kern-0.1em%      % Shortening on the right side
      }%
    }%
  }%
} 

\newcommand{\rmsc}[1]{\textrm{\textsc{#1}}}
\newcommand{\pkg}[1]{\textbf{#1}}
% \newcommand{\src}[1]{\texttt{#1}}

\newcommand{\pipe}{\texttt{\%>\%}}
\newcommand{\expipe}{\texttt{\%\$\%}}
\newcommand{\apipe}{\texttt{\%<>\%}}
\newcommand{\rpipe}{\texttt{|>}}

\title{Data Cleaning, Data Visualization, \& Functions}
\subtitle{Fundamental Techniques in Data Science}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Utrecht University}
\date{}

<<setup, include = FALSE, cache = FALSE>>=
set.seed(235711)

library(knitr)
library(dplyr)
library(magrittr)
library(xtable)

                                        #source("../../../code/supportFunctions.R")

options(width = 60)
opts_chunk$set(size = "footnotesize",
               fig.align = "center",
               fig.path = "figure/r_basics-",
               message = FALSE,
               warning = FALSE,
               comment = "")
knit_theme$set('edit-kwrite')
@

\begin{document}
  
%------------------------------------------------------------------------------%

\begin{frame}[t,plain]
  \titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%------------------------------------------------------------------------------%

\sectionslide{Functions}

%------------------------------------------------------------------------------%

\begin{frame}{R Functions}

  Functions are the foundation of R programming.
  \vc
  \begin{itemize}
  \item Other than data objects, almost everything else that you interact with
    when using R is a function.
    \vc
  \item Any R command written as a word followed by parentheses \src{()} is a
    function.
    \vc
    \begin{itemize}
    \item \src{mean()}
    \item \src{library()}
    \item \src{mutate()}
    \end{itemize}
    \vc
  \item Infix operators are aliased functions.
    \vc
    \begin{itemize}
    \item \src{<-}
    \item \src{+}, \src{-}, \src{*}
    \item \pipe, \expipe, \apipe
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{User-Defined Functions}
  
  We can define our own functions using the \src{function()} function.

  <<>>=
  square <- function(x) {
      out <- x^2
      out
  }
  @

  After defining a function, we use it in the same way as any other R function.

  <<>>=
  square(5)
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{User-Defined Functions}

  One-line functions don't need braces

  <<>>=
  square <- function(x) x^2
  square(5)
  @

  Function arguments are not strictly typed. R will try to work with whatever
  you provide as input.

  <<>>=
  square(1:5)
  square(pi)
  square(TRUE)
  square("bob") # But one can only try so hard
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{User-Defined Functions}
 
  Functions can take multiple arguments.
  
  <<>>=
  mod <- function(x, y) x %% y
  mod(10, 3)
  @
  
  Sometimes it's useful to specify a list of arguments that we unpack inside the
  function.

  <<>>=
  getLsBeta <- function(datList) {
      X <- datList$X
      y <- datList$y
      
      solve(crossprod(X)) %*% t(X) %*% y
  }
  
  X       <- matrix(runif(500), ncol = 5)
  datList <- list(y = X %*% rep(0.5, 5), X = X)
  
  getLsBeta(datList = datList)
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{User-Defined Functions}

  Functions are first-class objects in R.
  \begin{itemize}
  \item We can treat them like any other R object.
  \end{itemize}

  \vb
  
  R views an initialized, but unevaluated, function as a special object with
  type "closure"

  <<>>=
  class(getLsBeta)
  typeof(getLsBeta)
  @

  After evaluation, functions are simply equivelent to the objects they return.

  <<>>=
  class(getLsBeta(datList))
  typeof(getLsBeta(datList))
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{User-Defined Functions}

We can use functions as arguments to other operations and functions.

<<>>=
fun1 <- function(x, y) x + y

## What will this command return?
fun1(1, fun1(1, 1))
@

Why would we care?

<<>>=
s2 <- var(runif(100))
x  <- rnorm(100, 0, sqrt(s2))

X[1:10, ]

c(1, 3, 6:9, 12)
@

\end{frame}

%------------------------------------------------------------------------------%

\sectionslide{Data Visualization}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Setup}

  <<>>=
  dataDir  <- "../../../data/"
  
  diabetes <- readRDS(paste0(dataDir, "diabetes.rds"))
  titanic  <- readRDS(paste0(dataDir, "titanic.rds"))
  bfi      <- readRDS(paste0(dataDir, "bfi.rds"))
  
  ## Convert surival indicator to a numeric dummy code:
  titanic <- titanic %>% mutate(survived = as.numeric(survived) - 1)
  @

\end{frame}

%------------------------------------------------------------------------------%

\subsection{Base R Graphics}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Base R Graphics: Scatterplots}

  We can create a basic scatterplot using the \src{plot()} function.
  
  <<out.width = "50%">>=
  diabetes %$% plot(y = tc, x = bmi)
  @

  \pagebreak

  <<eval = FALSE>>=
  diabetes %$% plot(y = tc,
                    x = bmi,
                    ylab = "Total Cholesterol",
                    xlab = "Body Mass Index",
                    main = "Relation between BMI and Cholesterol",
                    ylim = c(0, 350),
                    xlim = c(0, 50)
                    )
  @

  \pagebreak

  <<echo = FALSE, out.width = "65%">>=
  diabetes %$% plot(y = tc,
                    x = bmi,
                    ylab = "Total Cholesterol",
                    xlab = "Body Mass Index",
                    main = "Relation between BMI and Cholesterol",
                    ylim = c(0, 350),
                    xlim = c(0, 50)
                    )
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Base R Graphics: Histograms}

  We can create a simple histogram with the \src{hist()} function.

  <<out.width = "50%">>=
  hist(diabetes$glu)
  @

  \pagebreak

  <<out.width = "50%">>=
  hist(diabetes$glu, breaks = 5)
  @

  \pagebreak

  <<out.width = "50%">>=
  hist(diabetes$glu, breaks = 50)
  @

  \pagebreak

  <<out.width = "50%">>=
  hist(diabetes$glu, col = "pink", border = "blue", probability = TRUE)
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Base R Graphics: Boxplots}

  We can create simple boxplots via the \src{boxplot()} function.

  <<out.width = "50%">>=
  boxplot(diabetes$progress)
  @

  \pagebreak

  <<eval = FALSE>>=
  boxplot(diabetes$progress,
          horizontal = TRUE,
          range = 3,
          xlab = "Disease Progression")
  @

  \pagebreak

  <<echo = FALSE, out.width = "65%">>=
  boxplot(diabetes$progress,
          horizontal = TRUE,
          range = 3,
          xlab = "Disease Progression")
  @

  \pagebreak

  <<out.width = "50%">>=
  boxplot(progress ~ sex, data = diabetes, col = "violet")
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Base R Graphics: Fancy Things}

  Plotting an entire data frame produces a scatterplot matrix.
 
  <<out.width = "50%">>=
  diabetes %>% select(age, bmi, tc, glu, progress) %>% plot()
  @

  \pagebreak

  The \src{density()} function estimates the density of a variable.
  \begin{itemize}
  \item If we plot a density object, we get a kernel density plot.
  \end{itemize}
    
  <<out.width = "40%">>=
  density(diabetes$bmi) %>% plot()
  @

  \pagebreak

  <<>>=
  d <- density(diabetes$bmi)
  ls(d)
  @

  \pagebreak

  <<out.width = "50%">>=
  d %$% plot(y = y, x = x, type = "l")
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Base R Graphics: Workflow}

  Base R graphics work by building up graphics from layers.

  <<eval = FALSE>>=
  ## Start with a simple scatterplot:
  diabetes %$% plot(y = tc, x = bmi, pch = 20, xlab = "", ylab = "")
  
  ## Use the abline() function to add lines representing the means of x and y:
  abline(h = mean(diabetes$tc), v = mean(diabetes$bmi), lty = 2)

  ## Add the best fit line from a linear regression of 'tc'  onto 'bmi':
  diabetes %$%
      lm(tc ~ bmi) %>%
      coef() %>%
      abline(coef = ., col = "blue", lwd = 2)
  
  ## Add titles:
  title(main = "Total Cholesterol by Body Mass Index",
        ylab = "Total Cholesterol",
        xlab = "Body Mass Index")
  @
  
  \pagebreak
  
  <<echo = FALSE, out.width = "65%">>=
  diabetes %$% plot(y = tc, x = bmi, pch = 20, xlab = "", ylab = "")
  
  abline(h = mean(diabetes$tc), v = mean(diabetes$bmi), lty = 2)

  diabetes %$%
      lm(tc ~ bmi) %>%
      coef() %>%
      abline(coef = ., col = "blue", lwd = 2)
  
  title(main = "Total Cholesterol by Body Mass Index",
        ylab = "Total Cholesterol",
        xlab = "Body Mass Index")
  @

\end{frame}

% ------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Base R Graphics: Workflow}
  
  Add a kernel density plot on top of a histogram.

  <<eval = FALSE>>=
  diabetes %$%
      hist(age,
           probability = TRUE,
           xlab = "Age",
           main = "Distribution of Age")

  diabetes %$%
      density(age) %>%
      lines(col = "red", lwd = 2)
  @

  \pagebreak
  
  <<echo = FALSE, out.width = "65%">>=
  diabetes %$%
      hist(age,
           probability = TRUE,
           xlab = "Age",
           main = "Distribution of Age")

  diabetes %$%
      density(age) %>%
      lines(col = "red", lwd = 2)
  @

\end{frame}

%------------------------------------------------------------------------------%

\subsection{GGPlot}

%------------------------------------------------------------------------------%

\begin{frame}{GGPlot}

  Base R graphics are fine for quick-and-dirty visualizations (e.g., EDA,
  checking assumptions), but for publication quality graphics, we probably want
  to use GGPlot.

  GGPlot uses the "grammar of graphics" and "tidy data" to build up a figure
  from modular components

  \includegraphics[width = 0.8\textwidth]{images/grammar.jpg}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{GGPlot: Basic Setup}

  We start by calling the \src{ggplot()} function.
  \begin{itemize}
  \item We must define a data source.
  \item We must also give some aesthetic via the \src{aes()} function.
  \end{itemize}
  
  <<>>=
  library(ggplot2)
  p1 <- ggplot(data = diabetes, mapping = aes(x = bmi, y = glu))
  @

  \pagebreak

  At this point, our plot is pretty boring.

  <<out.width = "50%">>=
  p1
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{GGPlot: Geometries}
  
  We need to define some geometry via an appropriate \src{geom_X()} function.

  <<out.width = "50%">>=
  p1 + geom_point()
  @

  \pagebreak

  <<out.width = "50%">>=
  p1 + geom_line()
  @

  \pagebreak

  <<out.width = "50%">>=
  p1 + geom_rug()
  @

  \pagebreak
  
  We can also combine different geoms into a single figure

  <<out.width = "50%">>=
  p1 + geom_point() + geom_line() + geom_rug()
  @

  \pagebreak
  
  We can use different flavors of geom for different types of data

  <<out.width = "50%">>=
  p2 <- ggplot(diabetes, aes(tc))
  p2 + geom_histogram()
  @

  \pagebreak

  <<out.width = "50%">>=
  p2 + geom_density()
  @

  \pagebreak

  <<out.width = "50%">>=
  p2 + geom_boxplot()
  @

  \pagebreak

  <<out.width = "50%">>=
  p3 <- ggplot(diabetes, aes(sex, bmi))
  p3 + geom_boxplot()
  @
  
  \pagebreak

  <<out.width = "50%">>=
  p3 + geom_violin()
  @

  \pagebreak

  <<out.width = "50%">>=
  p4 <- ggplot(bfi, aes(education, age))
  p4 + geom_point()
  @

  \pagebreak

  <<out.width = "50%">>=
  p4 + geom_jitter()
  @
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{GGPlot: Statistics}
    
  We can also add statistical summaries of the data
  
  <<out.width = "50%">>=
  p1 + geom_point() + geom_smooth()
  @

  \pagebreak

  <<out.width = "50%">>=
  p1 + geom_point() + geom_smooth(method = "lm")
  @
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{GGPlot: Styling}

  Changing style options outside of the \src{aes()} function applies the styling
  to the entire plot.

  <<out.width = "50%">>=
  p5 <- ggplot(titanic, aes(age, survived))
  p5 + geom_jitter(color = "blue", size = 3, height = 0.1)
  @
  
  \pagebreak
  
  We can also apply styles as a function of variables by defining the style
  within the \src{aes()} function.

  <<out.width = "50%">>=
  p6.1 <- ggplot(titanic, aes(age, survived, color = sex))
  p6.1 + geom_jitter(size = 3, height = 0.1) + geom_smooth()
  @

  \pagebreak

  <<out.width = "50%">>=
  p6.2 <- ggplot(titanic, aes(age, survived))
  p6.2 + geom_jitter(aes(color = sex), size = 3, height = 0.1) +
      geom_smooth()
  @

  \pagebreak

  <<out.width = "50%">>=
  p6.2 + geom_jitter(size = 3, height = 0.1) +
      geom_smooth(aes(color = sex))
  p6.2 + geom_jitter(aes(color = class), size = 3, height = 0.1) +
      geom_smooth(aes(color = sex))
  @

  \pagebreak

  <<out.width = "50%">>=
  p6.2 + geom_jitter(aes(shape = class), size = 3, height = 0.1) +
    geom_smooth(aes(color = sex))
  @

  \pagebreak

  <<out.width = "50%">>=
  p6.1 + geom_jitter(aes(shape = class), size = 3, height = 0.1) +
    geom_smooth()
  @
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{GGPlot: Themes}

  We can apply several pre-baked themes to adjust a plot's overall appearance

  <<out.width = "50%">>=
  (p1.1 <- p1 + geom_point())
  @

  \pagebreak

  <<out.width = "50%">>=
  p1.1 + theme_classic()
  @

  \pagebreak

  <<out.width = "50%">>=
  p1.1 + theme_minimal()
  @

  \pagebreak

  <<out.width = "50%">>=
  p1.1 + theme_bw()
  @

  \pagebreak

  We can also moodifying individual theme elements.

  <<eval = FALSE>>=
  p1.1 + theme_classic() +
      theme(axis.title = element_text(size = 16,
                                      family = "serif",
                                      face = "bold",
                                      color = "blue"),
            aspect.ratio = 1)
  @

  \pagebreak

  <<echo = FALSE, out.width = "65%">>=
  p1.1 + theme_classic() +
      theme(axis.title = element_text(size = 16,
                                      family = "serif",
                                      face = "bold",
                                      color = "blue"),
            aspect.ratio = 1)
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{GGPlot: Facets}
  
  Facetting allow us to make arrays of conditional plots.

  <<eval = FALSE>>=
  (p7 <- ggplot(titanic, aes(age, survived, color = class)) +
       geom_jitter(height = 0.05) +
       geom_smooth(method = "glm",
                   method.args = list(family = "binomial"),
                   se = FALSE)
  )
  @

  \pagebreak

  <<echo = FALSE, out.width = "65%">>=
  (p7 <- ggplot(titanic, aes(age, survived, color = class)) +
       geom_jitter(height = 0.05) +
       geom_smooth(method = "glm",
                   method.args = list(family = "binomial"),
                   se = FALSE)
  )
  @

  \pagebreak

  <<out.width = "50%">>=
  p7 + facet_wrap(vars(sex))
  @

  \pagebreak

  <<eval = FALSE>>=
  ## Use facet_grid() to condition plots on both 'sex' and 'class'
  (p8 <- ggplot(titanic, aes(age, survived)) +
       geom_jitter(height = 0.05) +
       geom_smooth(method = "glm",
                   method.args = list(family = "binomial"),
                   se = FALSE)
  )
  @

  \pagebreak

  <<echo = FALSE, out.width = "65%">>=
  (p8 <- ggplot(titanic, aes(age, survived)) +
       geom_jitter(height = 0.05) +
       geom_smooth(method = "glm",
                   method.args = list(family = "binomial"),
                   se = FALSE)
  )
  @

  \pagebreak
  
  <<echo = FALSE, out.width = "65%">>=
  p8 + facet_grid(vars(sex), vars(class))
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{GGPlot: Joining Multiple Figures}

  If we want to paste several different plots into a single figure (without
  facetting), we can use the utilities in the \pkg{gridExtra} package.
  
  <<eval = FALSE>>=
  library(gridExtra)
  
  grid.arrange(p1 + geom_point(),
               p3 + geom_boxplot(),
               p4 + geom_jitter(),
               p8 + facet_grid(vars(sex), vars(class)),
               ncol = 2)
  @
  
  \pagebreak

  <<echo = FALSE, out.width = "65%">>=
  library(gridExtra)
  
  grid.arrange(p1 + geom_point(),
               p3 + geom_boxplot(),
               p4 + geom_jitter(),
               p8 + facet_grid(vars(sex), vars(class)),
               ncol = 2)
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Saving Graphics}

  To save a graphic that we've created in R, we simply redirect the graphical
  output to a file using an appropriate function.

  <<>>=
  figDir <- "figures/"
  
  ## Save as PDF
  pdf(paste0(figDir, "example_plot.pdf"))

  p7 + facet_wrap(vars(sex))
  
  dev.off()
  @

  \pagebreak

  <<>>=
  ## Save as JPEG
  jpeg(paste0(figDir, "example_plot.jpg"))
  
  p7 + facet_wrap(vars(sex))
  
  dev.off()
  @

  \pagebreak

  <<>>=
  ## Save as PNG
  png(paste0(figDir, "example_plot.png"))
  
  p7 + facet_wrap(vars(sex))
  
  dev.off()
  @

  \pagebreak
  
  With PDF documents, we can save multiple figures to a single file.

  <<>>=
  pdf(paste0(figDir, "example_plot2.pdf"))
  
  p6.1 + geom_jitter(size = 3, height = 0.1) + geom_smooth()
  p7 + facet_wrap(vars(sex))
  p8 + facet_grid(vars(sex), vars(class))
  
  dev.off()
  @

\end{frame}

%------------------------------------------------------------------------------%

\sectionslide{Data Cleaning}

%------------------------------------------------------------------------------%

\subsection{Data Analytic Lifecycle}

%------------------------------------------------------------------------------%

\begin{frame}{Research Cycle}

  The following is a representation of the \emph{Research Cycle} used for 
  empirical research in most of the sciences.
  
  \begin{figure}
    \includegraphics[width = 0.7\textwidth]{figures/research_cycle.pdf}
  \end{figure}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{CRISP-DM}

  The \emph{Cross-industry Standard Process for Data Mining} was developed to 
  standardized the process of data mining in industry applications.
  
  \begin{figure}
    \includegraphics[width = 0.7\textwidth]{figures/crisp-dm.pdf}
  \end{figure}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Data Science Cycle}
  
  The \emph{Data Science Cycle} represented here was adapted from 
  \citet{o'neilSchutt:2014}.
  
  \begin{figure}
    \includegraphics[width = 0.7\textwidth]{figures/data_science_cycle.pdf}
  \end{figure}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Data Cleaning}
  
  When we receive new data, they are generally messy and contaminated by various 
  anomalies and errors.
  \vb
  \begin{itemize}
  \item One of the first steps in processing a new set of data is 
    \emph{cleaning}.
    \vc
  \item By cleaning the data, we ensure a few properties:
    \vc
    \begin{itemize}
    \item The data are in an analyzable format.
      \vc
    \item All data take legal values.
      \vc
    \item Any outliers are located and treated.
      \vc
    \item Any missing data are located and treated.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\subsection{Missing Data}

%------------------------------------------------------------------------------%

\begin{frame}{What are Missing Data?}
  
  Missing data are empty cells in a dataset where there should be observed 
  values.
  \vc
  \begin{itemize}
  \item The missing cells correspond to true population values, but we haven't 
    observed those values.
  \end{itemize}
  \vb 
  \pause
  Not every empty cell is a missing datum.
  \vc
  \begin{itemize}
  \item Quality-of-life ratings for dead patients in a mortality study
    \vc
  \item Firm profitability after the company goes out of business
    \vc
  \item Self-reported severity of menstrual cramping for men
    \vc
  \item Empty blocks of data following ``gateway'' items
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%
    
\begin{frame}
  
  \begin{center}
    \huge{Missing Data Descriptives}
  \end{center}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\captionsetup{labelformat = empty}

\begin{frame}{Missing Data Pattern}
  
<<echo = FALSE>>=
tmpTab <- matrix(c("x", "x", ".", ".",
                   "y", ".", "y", "."),
                 ncol = 2,
                 dimnames = list(NULL, c("X", "Y"))
                 )

patTab1 <- xtable(tmpTab, align = rep("c", 3), caption = "Patterns for $P = 2$")

tmpTab <- matrix(c(rep("x", 3), ".", "x", rep(".", 3),
                   "y", "y", ".", "y", ".", ".", "y", ".",
                   "z", ".", "z", "z", ".", "z", ".", "."),
                 ncol = 3,
                 dimnames = list(NULL, c("X", "Y", "Z"))
                 )

patTab2 <- xtable(tmpTab, align = rep("c", 4), caption = "Patterns for $P = 3$")
@ 

Missing data (or response) patterns represent unique combinations of observed
and missing items.
\begin{itemize}
  \item $P$ items $\Rightarrow$ $2^P$ possible patterns.
\end{itemize}

\begin{columns}
  \begin{column}{0.45\textwidth}
    
<<echo = FALSE, results = 'asis'>>=
print(patTab1, booktabs = TRUE)
@ 
    
  \end{column}
  \begin{column}{0.45\textwidth}
\vx{-12}    
<<echo = FALSE, results = 'asis'>>=
print(patTab2, booktabs = TRUE)
@ 
     
    \end{column}
  \end{columns}
  
\end{frame}

\captionsetup{labelformat = default}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Nonresponse Rates}
  
  \textsc{Percent/Proportion Missing}
  \begin{itemize}
  \item The proportion of cells containing missing data
  \item Should be computed for each variable, not for the entire dataset
  \end{itemize}
  
  \vb
  
  \textsc{Attrition Rate}
  \begin{itemize}
  \item The proportion of participants that drop-out of a study at each 
    measurement occasion
  \end{itemize}
  
  \vb
  
  \textsc{Percent/Proportion of Complete Cases}
  \begin{itemize}
  \item The proportion of observations with no missing data
  \item Often reported but nearly useless quantity
  \end{itemize}
  
  \vb
  
  \textsc{Covariance Coverage}
  \begin{itemize}
  \item The proportion of cases available to estimate a given pairwise
    relationship (e.g., a covariance between two variables)
  \item Very important to have adequate coverage for the parameters you want to 
    estimate
  \end{itemize}
    
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

  We can calculate basic response rates with simple base R commands.

  <<>>=
  ## Load some example data:
  data(boys, package = "mice")

  ## Compute variable-wise proportions missing:
  mMat  <- is.na(boys)
  mMat %>% colMeans() %>% round(3)
  @

  \pagebreak

  <<>>=
  ## Compute observation-wise proportions missing:
  pmRow <- rowMeans(mMat)

  ## Summarize the above:
  range(pmRow)
  range(pmRow[pmRow > 0])
  median(pmRow)

  ## Compute the proportion of complete cases:
  mean(pmRow == 0)
  @
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

  We can use routines from the \pkg{mice} package to calculate covariance
  coverage and response patterns.

  <<>>=
  ## Compute the covariance coverage:
  cc <- mice::md.pairs(boys)$rr / nrow(boys)

  ## Check the result:
  round(cc, 2)
  @

  \pagebreak
  
  <<>>=
  ## Range of coverages:
  range(cc)
  range(cc[cc < 1])

  ## How many coverages fall below some threshold?
  (cc[lower.tri(cc)] < 0.7) %>% sum()
  @

  \pagebreak

  <<out.width = "50%">>=
  ## Compute missing data patterns:
  pats <- mice::md.pattern(boys)
  @

  \pagebreak

  <<>>=
  pats
  @

  \pagebreak

  <<>>=
  ## How many unique response patterns?
  nrow(pats) - 1

  ## What is the most commond response patterns?
  maxPat <- rownames(pats) %>% as.numeric() %>% which.max()
  pats[maxPat, ]
  @
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Visualizing Incomplete Data}

  The \pkg{ggmice} package provides some nice ways to visualize incomplete data
  and objects created during missing data treatment.

  <<out.width = "40%">>=
  library(ggmice); library(ggplot2)
  
  ggmice(boys, aes(wgt, hgt)) + geom_point()
  @

  \pagebreak

  We can also create a nicer version of the response pattern plot.

  <<out.width = "50%">>=
  plot_pattern(boys, rotate = TRUE)
  @
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Visualizing Incomplete Data}

  The \pkg{naniar} package also provides some nice visualization and numerical
  summary routines for incomplete data.

  <<out.width = "40%">>=
  naniar::gg_miss_upset(boys)
  @
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\subsection{Outliers}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{What is an outlier?}
  
  For the time being, we're considering \emph{univariate outliers}.
  \vb
  \begin{itemize}
  \item Extreme values with respect to the distribution of a variable's other 
    observations
    \vc
    \begin{itemize}
    \item A human height measurement of 3 meters
      \vc
    \item A high temperature in Utrecht of $50^\circ$
      \vc
    \item Annual income of \euro250,000 for a student
    \end{itemize}
    \vb
  \item Not accounting for any particular model (we'll get to that later)
  \end{itemize}
  
  \pagebreak
  
  A univariate outlier may, or may not, be an illegal value.
  \vb
  \begin{itemize}
  \item Data entry errors are probably the most common cause.
    \vc
  \item Outliers can also be legal, but extreme, values.
  \end{itemize}
  
  \va
  
  \textsc{Key Point:} We choose to view an outlier as arising from a different 
  population than the one to which we want to generalize our findings.
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Finding Univariate Outliers}

  We have many methods available to diagnose potential outliers.
  \vb
  \begin{itemize}
  \item Four of the simplest and most popular are:
    \vc
    \begin{enumerate}
    \item Internally studentized residuals (AKA Z-score method)
      \vc
    \item Externally studentized residuals
      \vc
    \item Median absolute deviation method
      \vc
    \item Tukey's boxplot method
    \end{enumerate}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Internally Studentized Residuals}

  For each observation, $X_n$, we compute the following quantity:
  \begin{align*}
    T_n = \frac{X_n - \bigbar{X}}{SD_X}
  \end{align*}
  \vx{-12}
  \begin{itemize}
  \item $T_n$ follows a Student's $t$ distribution with $df = N - 1$.
    \begin{itemize}
    \item We can do a formal test for ``outlier'' status.
    \end{itemize}
  \item Assuming a large sample, if $T_n > C$ (where $C$ is usually 2 or 3), we 
    label $X_n$ as an outlier.
  \end{itemize}
  
  \vb
  \pause
  
  Although simple, this method has some substantial limitations.
  \begin{itemize}
  \item The cutpoint, $C$, can only be meaningfully chosen when $X$ is normally 
    distributed.
  \item Both $\bigbar{X}$ and $SD_X$ are highly sensitive to outliers.
  \end{itemize}
  
\end{frame}


%------------------------------------------------------------------------------%

\begin{frame}{Externally Studentized Residual}

  The externally studentized residual method is essentially the same as the 
  internally studentized residual method, but we adjust $\bigbar{X}$ and $SD_X$ 
  to remove the influence of the observation we're evaluating.
  \vb
  \begin{itemize}
  \item Let $\mathbb{N}_{(n)} = \{1, \ldots, (n - 1), (n + 1), \ldots, N\}$. 
  \item Define the deletion mean, $\bigbar{X}_{(n)}$, and deletion SD, 
    $SD_{X(n)}$, as:
    \begin{align*}
      \bigbar{X}_{(n)} &= \frac{1}{N - 1} \sum_{i \in \mathbb{N}_{(n)}} X_i\\
      SD_{X(n)} &= \sqrt{\frac{1}{N - 2} \sum_{i \in \mathbb{N}_{(n)}} \left(X_i - \bigbar{X}_{(n)}\right)^2}
    \end{align*}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Externally Studentized Residual}
  
  The externally studentized residual is defined in the same way as the 
  internally studentized version:
  \begin{align*}
    T_{(n)} = \frac{X_n - \bigbar{X}_{(n)}}{SD_{X(n)}}
  \end{align*}
  \vx{-12}
  \begin{itemize}
  \item $T_{(n)}$ follows a Student's $t$ distribution with $df = N - 2$.
    \begin{itemize}
    \item We can do a formal test for ``outlier'' status.
    \end{itemize}
  \item Assuming a large sample, if $T_{(n)} > C$ (where $C$ is usually 2 or 3), 
    we label $X_n$ as an outlier.
  \end{itemize}
  
  \vb
  \pause
  
  $T_{(n)}$ is immune to the influence of the $n$th observation.
  \begin{itemize}
  \item Still requires $X$ to be normally distributed
  \item Still sensitive to outliers other than the $n$th observation
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Median Absolute Deviation Method}
  
<<echo = FALSE>>=
q <- qnorm(0.75)
b <- 1 / q
@ 

The biggest limitation of studentized residuals is that their measures of 
central tendency and dispersion are sensitive to outliers.
\vb
\begin{itemize}
\item If we can replace the (deletion) mean and the (deletion) SD with more 
  robust statistics, we can avoid this issue.
  \vb
  \begin{itemize}
  \item Replace the mean, $\bar{X}$, with the \emph{median}, $\text{Med}(X)$
    \vb
  \item Replace the SD with the \emph{median absolute deviation}:
    \begin{align*}
      MAD_X = b \times \text{Med} \left( \big| X_n - \text{Med} (X) \big| 
      \right)
    \end{align*}
  \item We choose the coefficient as $b = 1/Q_{0.75}$
    \vb
  \item For the normal distribution, $b \approx 1/\Sexpr{round(q, 4)} \approx 
    \Sexpr{round(b, 4)}$
  \end{itemize}
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Median Absolute Deviation Method}
  
  We compute our test statistic by replacing the mean with the median and the SD 
  with the MAD in the standard Wald test formula:
  \begin{align*}
    T_{MAD} = \frac{X_n - \text{Med}(X)}{MAD_X}
  \end{align*}
  \vx{-12}
  \begin{itemize}
  \item $T_{MAD}$ doesn't allow for formal statistical tests.
  \item We can use the same general cutoffs we would use for the studentized 
    residual methods.
    \begin{itemize}
    \item Assuming a large sample, if $T_{(n)} > C$ (where $C$ is usually 2 or 
      3), we label $X_n$ as an outlier.
    \end{itemize}
  \end{itemize}
  
  \vb
  \pause
  
  $T_{MAD}$ is immune to the influence of, up to, 50\% outlying observations.
  \begin{itemize}
  \item Requires us to assume a parametric distribution for $X$
    \begin{itemize}
    \item This assumption is necessary to compute $b$.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Breakdown Point}
  
  To compare robust statistics, we consider their \emph{breakdown points}.
  \begin{itemize}
  \item The breakdown point is the minimum proportion of cases that must be 
    replaced by $\infty$ to cause the value of the statistic to go to $\infty$.
  \end{itemize}
  \vc
  The mean has a breakdown point of $1 / N$.
  \begin{itemize}
  \item Replacing a single value with $\infty$ will produce an infinite mean.
  \end{itemize}
  \vc
  The deletion mean has a breakdown point of $2 / N$.
  \begin{itemize}
  \item We can replace, at most, 1 value with $\infty$ without producing an 
    infinite mean.
  \end{itemize}
  \vc
  The median has breakdown point of 50\%.
  \begin{itemize}
    \item We can replace $n < N / 2$ of the observations with $\infty$ without 
      producing an infinite median.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Boxplot Method}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      \citet{tukey:1977} described a procedure for flagging potential outliers 
      based on the familiar box-and-whiskers plot.
      \begin{itemize}
      \item Does not require normally distributed $X$
      \item Not sensitive to outliers
      \item Doesn't allow for formal statistical tests
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
data(Salaries, package = "carData")

boxplot(salary ~ sex,
        data = Salaries,
        ylab = "Salary",
        main = "9-Month Salaries of Professors")
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Boxplot Method}
  
  A \emph{fence} is an interval defined as the following function of the 
  \emph{first quartile}, the \emph{third quartile}, and the \emph{inner quartile
    range} ($IQR = Q_3 - Q_1$):
  \begin{align*}
    F = \{Q_1 - C \times IQR, Q_3 + C \times IQR\}
  \end{align*}
  
  \vx{-6}
  
  \begin{itemize}
  \item Taking $C = 1.5$ produces the \emph{inner fence}.
  \item Taking $C = 3.0$ produces the \emph{outer fence}.
  \end{itemize}
  
  \vb
  
  We can use these fences to identify potential outliers:
  \begin{itemize}
  \item Any value that falls outside of the inner fence is a \emph{possible 
    outlier}.
  \item Any value that falls outside of the outer fence is a \emph{probable 
    outlier}.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Multivariate Outliers}
  
  Sometimes, the combinations of values in an observation are very unlikely, 
  even when no individual value is an outlier.
  \vc
  \begin{itemize}
  \item These observations are \emph{multivariate outliers}.
    \vc
    \begin{itemize}
    \item A person in the 95\emph{th} percentile for height and the 5\emph{th} 
      percentile for weight
      \vc
    \item A person who simultaneously scores highly on scales of depression and 
      positive affect
    \end{itemize}
  \end{itemize}
  \vb
  To detect multivariate outliers, we use \emph{distance metrics}.
  \vc
  \begin{itemize}
  \item Distance metrics quantify the similarity of two vectors.
    \vc
    \begin{itemize}
    \item Similarity between two observations
      \vc
    \item Similarity between an observation and the mean vector
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Mahalanobis Distance}
  
  One of the most common distance metrics is the \emph{Mahalanobis Distance}.
  \vc
  \begin{itemize}
  \item The Mahalanobis distance, $\Delta$, is a multivariate generalization of 
    the internally studentized residual:
  \end{itemize}
  \begin{align*}
    \Delta_n = \sqrt{\left( \mathbf{x}_n - \hat{\mu}_{\mathbf{X}} \right) 
      \hat{\Sigma}_{\mathbf{X}}^{-1} \left( \mathbf{x}_n - \hat{\mu}_{\mathbf{X}} 
      \right)^T}
  \end{align*}
  As with studentized residuals, if $\Delta_n > C$, we label $\mathbf{x}_n$ as 
  an outlier.
  \vc
  \begin{itemize}
  \item When $\mathbf{X}$ is $K$-variate normally distributed, $\Delta_n^2$ 
    follows a $\chi^2$ distribution with $df = K$.
    \vc
  \item We take $C$ to be the square-root of a suitably conservative quantile 
    (e.g., $q \in \{99\%, 99.9\%\}$) of the $\chi_K^2$ distribution: 
    $C = \sqrt{\chi_{K, q}^2}$.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Problems with Mahalanobis Distance}
  
  Like the internally studentized residual, Mahalanobis distance is highly 
  sensitive to outliers.
  \vc
  \begin{itemize}
  \item The underlying estimates of central tendency, $\hat{\mu}_{\mathbf{X}}$, 
    and dispersion, $\hat{\Sigma}_{\mathbf{X}}$, are computed using all 
    observations.
  \end{itemize}
  \vb
  \pause
  We want robust analogues of $\hat{\mu}_{\mathbf{X}}$ and 
  $\hat{\Sigma}_{\mathbf{X}}$.
  \vc
  \begin{itemize}
  \item We have several options for robust estimation of $\hat{\mu}_{\mathbf{X}}$ 
    and $\hat{\Sigma}_{\mathbf{X}}$. E.g.:
    \begin{itemize}
    \item Minimum covariance determinant method \citep[MCD; ][]{rousseeuw:1985}
      \vc
    \item Minimum volume ellipsoid method \citep[MVE; ][]{rousseeuw:1985}
      \vc
    \item M-estimation \citep{maronna:1976}
    \end{itemize}
    \vc
  \item Conceptually, robust methods operate by either:
    \begin{itemize}
    \item Using only a ``good'' subset of data to estimate $\hat{\mu}_{\mathbf{X}}$ 
      and $\hat{\Sigma}_{\mathbf{X}}$.
      \vc
    \item Downweighting outlying observations when estimating 
      $\hat{\mu}_{\mathbf{X}}$ and $\hat{\Sigma}_{\mathbf{X}}$.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Robust Mahalanobis Distance}
  
  Equipped with robust estimates of central tendency, $\hat{\mu}_{R,\mathbf{X}}$, 
  and dispersion, $\hat{\Sigma}_{R,\mathbf{X}}$, we define the robust Mahalanobis 
  distance in the natural way:
  \begin{align*}
    \Delta_{R,n} = \sqrt{\left( \mathbf{x}_n - \hat{\mu}_{R,\mathbf{X}} \right) 
      \hat{\Sigma}_{R,\mathbf{X}}^{-1} \left( \mathbf{x}_n - \hat{\mu}_{R,\mathbf{X}} 
      \right)^T}
  \end{align*}
  We use $\Delta_{R,n}$ in the same way as $\Delta_n$.
  \vc
  \begin{itemize}
  \item If $\Delta_{R,n} > C$, we label $\mathbf{x}_n$ as an outlier.
    \vc
  \item Again, we take $C$ to be the square-root of some quantile of the 
    $\chi_K^2$ distribution: $C = \sqrt{\chi_{K, q}^2}$.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Practicalities: Univariate vs. Multivariate}

  Univariate outlier checks are safe for most variables.\\ 
  \vb
  \pause
  \textsc{\underline{Don't}} include too many variables in multivariate outlier 
  checks.
  \begin{itemize}
  \item More variables increases the chances of false positives.
  \item E.g., don't run a multivariate outlier test on your entire dataset.
  \end{itemize}
  \vb
  \pause
  \textsc{\underline{Do}} use multivariate outlier checks for scales.
  \begin{itemize}
  \item E.g., if you have a psychometric scale measuring depression, you should
    check the items of that scale for multivariate outliers.
  \end{itemize}
  \vb 
  \pause
  \textsc{\underline{Maybe}} check the variables in a single model for
  multivariate outliers.
  \begin{itemize}
  \item E.g., if you have a small set of items that you will include in a
    regression model, it could make sense to check these variables for
    multivariate outliers.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Practicalities: Outliers for Categorical Data}

  Nominal, ordinal, and binary items \emph{can} have outliers.
  \begin{itemize}
  \item Outliers on categorical variables are often more indicative of bad
    variables than outlying cases.
  \end{itemize}
  \vb
  \pause
  Ordinal
  \begin{itemize}
  \item Most participant endorse one of the lowest categories on an ordinal
    item, but a few participants endorse the highest category.
  \item The participants who endorse the highest category may be outliers.
  \end{itemize}
  \vb
  \pause
  Nominal
  \begin{itemize}
  \item Groups with very low membership may be outliers on nominal grouping
    variables.
  \end{itemize}
  \vb
  \pause
  Binary
  \begin{itemize}
  \item If most endorse the item, the few who do not may be outliers.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Treating Outliers}
  
  If we locate any outliers, they must be treated.
  \vc
  \begin{itemize}
  \item Outliers cause by errors, mistakes, or malfunctions (i.e., \emph{error 
    outliers}) should be directly corrected.
    \vc
  \item Labeling non-error outliers is a subjective task.
    \begin{itemize}
    \item A (non-error) outlier must originate from a population separate from 
      the one we care about.
    \item Don't blindly automate the decision process.
    \end{itemize}
  \end{itemize}
  
  \pause
  \vb
 
  The most direct solution is to delete any outlying observation.
  \vc
  \begin{itemize}
  \item If you delete non-error outliers, the analysis should be reported twice: 
    with outliers and without.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Treating Outliers}
  
  For univariate outliers, we can use less extreme types of deletion.
  \begin{itemize}
  \item Delete outlying values (but not the entire observation).
  \item These empty cells then become missing data.
  \end{itemize}
  \vb
  Winsorization:
  \begin{itemize}
  \item Replace the missing values with the nearest non-outlying value.
  \end{itemize}
  \vb
  Missing data analysis:
  \begin{itemize}
  \item Treat the missing values along with any naturally-occurring nonresponse.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Treating Outliers}
  
  We can also use robust regression procedures to estimate the model directly in 
  the presence of outliers.
  \vc
  \begin{itemize}
  \item Weight the objective function to reduce the impact of outliers
    \begin{itemize}
    \item M-estimation
    \end{itemize}
    \vc
  \item Trim outlying observations during estimation 
    \begin{itemize}
    \item Least trimmed squares, MCD, MVE
    \end{itemize}
    \vc
  \item Take the median, instead of the mean, of the squared residuals
    \begin{itemize}
    \item Least median of squares
    \end{itemize}
    \vc
  \item Model some quantile of the DV's distribution instead of the mean 
    \begin{itemize}
    \item Quantile regression
    \end{itemize}
    \vc
  \item Model the outcome with a heavy-tailed distribution
    \begin{itemize}
    \item Laplacian, Student's T
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{References}

  \bibliographystyle{apacite}
  \bibliography{../../../bibtex/stat_meth_refs.bib}

\end{frame}

%------------------------------------------------------------------------------%

\end{document}
