---
title: "More Logistic regression"
author: "Kyle M. Lang"
date: "Fundamental Techniques in Data Science with R"
output:
  ioslides_presentation:
    smaller: yes
    widescreen: no
   # logo: logo.png
---


## Packages Used

```{r message = FALSE, warning = FALSE}
library(magrittr) # pipes
library(dplyr)    # data manipulation
library(mice)     # data
library(lattice)  # plotting - used for conditional plotting
library(ggplot2)  # plotting
library(DAAG)     # data sets and functions
library(caret)    # confusion matrices

source("../../code/supportFunctions.R")

## Make ggplot backgrounds transparent:
theme_update(plot.background = element_rect(fill = "transparent", colour = NA))

knitr::opts_chunk$set(dev.args = list(bg = "transparent"), 
                      dev = "svg",
                      warning = FALSE,
                      message = FALSE)
```


## Recap

With **logistic regression** we can model a binary outcome using a set of 
continuous and/or categorical predictors. 

- If we use **linear regression** to model a binary outcome variable, we would 
create a *linear probability model*.

The linear probability model is a (bad) way to describe the conditional 
probabilities.

- The residual variance is not constant (violation of homoscedasticity)
- The residuals are not normally distributed

Because the linear probability model violates some assumptions of the linear 
model, the standard errors and hypothesis tests are not valid. 

- In short, you may draw invalid conclusions.  


# Titanic data


## Example: Titanic Data

We will begin this lecture with a data set that records the survival of the
passengers on the maiden voyage of the Titanic.

```{r}
titanic <- readRDS("data/titanic.rds")
titanic %>% head()
```


## Inspect the Data

```{r}
str(titanic)
```


## Available Variables

The outcome/dependent variable:

- *survived*: Did the passenger survive?

Some potential predictors:

- *sex*: The passenger's sex 
- *class*: The class of the passenger's ticket
- *age*: The passenger's age in years
- *siblings_spouses*: Number of siblings + spouses traveling with the passenger
- *parents_children*: Number of parents + children traveling with the passenger


## Potential Hypotheses

We can investigating patterns in these data that relate to the survival 
probability. 

Based on the creed "women and children first", we could hypothesize that 

- `age` relates to the probability of survival $\rightarrow$ younger passengers 
have a higher probability of survival
- `sex` relates to the probability of survival $\rightarrow$ females have a 
higher probability of survival

Based on socioeconomic status, we could hypothesize that

- `class` relates to the probability of survival $\rightarrow$ passengers 
traveling with higher classes have a higher probability of survival


# A quick investigation


## Process the Data

Add a numeric version of the outcome:
```{r}
titanic <- mutate(titanic, survived_dummy = as.numeric(survived) - 1)
```

Split the data
```{r}
set.seed(235711)
filter <- c(rep(TRUE, 800), rep(FALSE, nrow(titanic) - 800)) %>% sample()
train  <- titanic[filter, ]
test   <- titanic[!filter, ]
```


## Is `age` related to `survival`?

```{r fig.height = 3}

ggplot(train, aes(x = age, y = survived_dummy)) + 
  geom_point() +
  geom_smooth(method = "glm", 
              method.args = list(family = "binomial"), 
              se = FALSE)
```


## Check surival rates by class

```{r}
train %$% table(class, survived)
```

Higher classes seem to have higher probabilities of survival.

- Converting the counts to marginal proportions will clarify the story.

```{r}
train %$% 
  table(class, survived) %>% 
  prop.table(margin = 1) %>% 
  round(2)
```


# Modeling Surival Probability


## Conditional Distributions of Age

```{r fig.height = 3}
train %$% histogram(~ age | survived)
```

The distribution of `age` for survivors is different from the distribution of 
`age` for non-survivors. 

- There is a point-mass for young survivors $\Rightarrow$ children have higher 
chances of survival. 

## Model Estimates

```{r}
glm(survived ~ age, data = train, family = "binomial") %>% summary()
```


## Conditional Distributions of Sex

```{r fig.height = 3}
train %$% histogram(~ sex | survived)
```

These distributions are very different! 

- Females seem to have a much higher probability of survival. 


## Model Estimates

```{r fig.height = 3}
glm(survived ~ sex, data = train, family = "binomial") %>% summary()
```


## Conditional Distributions of Class

```{r fig.height=3}
train %$% histogram(~ class | survived)
```

There is an obvious difference between the distributions of survivors and 
non-survivors over the classes. 

- In 1st and 2nd class, there are more survivors than non-survivors
- In 3rd class the relation reversed 


## Model Estimates

```{r}
glm(survived ~ class, data = train, family = "binomial") %>% summary()
```


## Multiple Logistic Regression Model

```{r}
fit1 <- glm(survived ~ age + sex + class, data = train, family = "binomial")
partSummary(fit1, -1)
```


## Add Interactions

```{r}
fit2 <- glm(survived ~ age * sex + age * class + sex * class, 
            data = train,
            family = "binomial")
summary(fit2)$coefficients
```


## Model Comparison

Test the change in deviance.
```{r}
anova(fit1, fit2, test = "Chisq") 
```


## Model Comparison

Compare information criteria 
```{r}
AIC(fit1, fit2)
BIC(fit1, fit2)
```


## Cross Validation

```{r}
set.seed(235711)
cv1 <- CVbinary(fit1, nfolds = 10)

set.seed(235711)
cv2 <- CVbinary(fit2, nfolds = 10)

c(fit1 = cv1$acc.cv, fit2 = cv2$acc.cv)
```


# Assumptions


## Assumptions of Logistic Regression

We can state the assumptions of linear regression as follows:

1. The outcome follows a binomial distribution.
1. The predictor matrix is full-rank.
1. The predictors are linearly related to $logit(\pi)$.
1. The observations are independent after accounting for the predictors.

Unlike linear regression, we don't need to assume

- Constant, finite error variance
- Normally distributed errors

For computational reasons, we also need the following:

- Large sample
- Relatively well-balance outcome
- No highly influential cases


## Non-Constant Variance

The mean of the binomial distribution is the success probability: $\pi$.

The variance is a function of the mean: $\textrm{var}(Y) = \pi (1 - \pi)$.

```{r, echo = FALSE}
eta <- predict(fit2, type = "link")
var <- plogis(eta) * (1 - plogis(eta))
dat <- data.frame(eta, var)

ggplot(dat, aes(x = eta, y = var)) + 
  geom_point() +
  ylab("Variance") +
  xlab("Linear Predictor")
```


## Residuals in Logistic Regression

There are many ways to define a residual in logistic regression.

- Response residuals
   - $\hat{\varepsilon}_{r,i} = Y_i - \hat{\pi}_i$
   - Direct analogue of residuals in linear regression
   - Not very useful for logistic regression
   
- Pearson residuals
   - $\hat{\varepsilon}_{p,i} = \frac{\hat{\varepsilon}_{r,i}}{\sqrt{\hat{\pi}_i(1 - \hat{\pi}_i)}}$
   - Addresses heterogeneity by dividing out the variance of the $i$th observation
   
- Deviance residuals
   - $\hat{\varepsilon}_{d,i} = \textrm{sign}(\hat{\varepsilon}_{r,i}) \sqrt{d_i}$
   - Most natural type of residual for logistic regression
   - Based on the objective function being optimized to estimate the model
   

## Visualizing Different Residuals

```{r, eval = FALSE}
rr <- resid(fit2, type = "response")
rp <- resid(fit2, type = "pearson")
rd <- resid(fit2, type = "deviance")

eta <- predict(fit2, type = "link")

rDat <- data.frame(Residual = c(rr, rp, rd),
                   Eta      = rep(eta, 3),
                   Type     = rep(c("Response", "Pearson", "Deviance"), 
                                  each = length(eta)
                                  )
                   )

ggplot(rDat, aes(x = Eta, y = Residual, color = Type)) + 
  geom_point(alpha = 0.35) +
  geom_smooth(se = FALSE)
```


## Visualizing Different Residuals

```{r, echo = FALSE}
rr <- resid(fit2, type = "response")
rp <- resid(fit2, type = "pearson")
rd <- resid(fit2, type = "deviance")

eta <- predict(fit2, type = "link")

rDat <- data.frame(Residual = c(rr, rp, rd),
                   Eta      = rep(eta, 3),
                   Type     = rep(c("Response", "Pearson", "Deviance"), 
                                  each = length(eta)
                                  )
                   )

ggplot(rDat, aes(x = Eta, y = Residual, color = Type)) + 
  geom_point(alpha = 0.35) +
  geom_smooth(se = FALSE)
```


## Diagnostics: Linearity

```{r}
plot(fit2, 1) # Pearson residuals
```


## Diagnostics: Influential Cases

```{r}
plot(fit2, 4)
```


## Diagnostics: Influencial Cases

```{r}
plot(fit2, 5)
```


# Classification


## Generate Predictions on the Logit Scale

```{r}
etaHat <- predict(fit2, newdata = test, type = "link", se.fit = TRUE)
sapply(etaHat, head)
```


## Generate Predicted Probabilities

Calculate $\hat{\pi}$ directly using the `predict()` function:
```{r}
piHat <- predict(fit2, newdata = test, type = "response", )
```

Apply the logistic function, `plogis()`, to the $\hat{\eta}$ values we computed 
earlier:
```{r}
piHat2 <- plogis(etaHat$fit)
head(cbind(piHat, piHat2))
```


## Generate Classifications

```{r}
yHat <- ifelse(piHat > 0.5, "yes", "no") %>% factor()
table(yHat)
```

Calculate the confusion matrix

```{r}
cm <- table(pred = yHat, true = test$survived)
cm
```


## Sensitivity, Specificity, & Accuracy
```{r}
(sensitivity <- cm["yes", "yes"] / sum(cm[ , "yes"]))
(specificity <- cm["no", "no"] / sum(cm[ , "no"]))
(acc <- diag(cm) %>% sum() / sum(cm))
```

We can also use the *caret::confusionMatrix()* function:
```{r eval = FALSE}
confusionMatrix(yHat, reference = test$survived)
```


## Output from confusionMatrix() {.smaller}

```{r, echo = FALSE}
confusionMatrix(yHat, reference = test$survived)
```


# Visualization


## Augment the Data

First, we'll add the predicted quantities to the testing data.

- We'll need these for plotting.

```{r}
test$eta <- etaHat$fit
test$se  <- etaHat$se.fit
test$pi  <- piHat
```

Next, we add confidence intervals for the predictions.

```{r}
test %<>% 
  mutate(etaLower = eta - 1.96 * se, 
         etaUpper = eta + 1.96 * se,
         piLower = plogis(etaLower),
         piUpper = plogis(etaUpper)
         )
```


## Visualizing the Predictions (Logit)

```{r fig.height = 3.5}
ggplot(test, aes(x = age, y = eta)) + 
  geom_line(aes(color = class), lwd = 1) +
  geom_ribbon(aes(ymin = etaLower, ymax = etaUpper, fill = class), alpha = 0.2) +
  ylab("Logit of Survival") +
  facet_wrap(vars(sex))
```


## Visualizing the Predictions (Probability)

```{r fig.height = 3.5}
ggplot(test, aes(x = age, y = pi)) + 
  geom_ribbon(aes(ymin = piLower, ymax = piUpper, fill = class), alpha = 0.2) +
  geom_line(aes(color = class), lwd = 1) + 
  ylab("Probability of Survival") +
  facet_wrap(vars(sex))
```


## Additive Model

Augment the data with predictions from the additive model:

```{r}
tmp <- predict(fit1, newdata = test, type = "link", se = TRUE)
test$eta4 <- tmp$fit
test$se4  <- tmp$se.fit
test$pi4  <- plogis(tmp$fit)

test %<>% 
  mutate(etaLower4 = eta4 - 1.96 * se4, 
         etaUpper4 = eta4 + 1.96 * se4,
         piLower4 = plogis(etaLower4),
         piUpper4 = plogis(etaUpper4)
         )
```


## Additive Model (Logit)

```{r fig.height = 3.5}
ggplot(test, aes(x = age, y = eta4)) + 
  geom_line(aes(color = class), lwd = 1) +
  geom_ribbon(aes(ymin = etaLower4, ymax = etaUpper4, fill = class), alpha = 0.2) +
  ylab("Logit of Survival") +
  facet_wrap(vars(sex))
```


## Additive Model (Probability)

```{r fig.height=3.5}
ggplot(test, aes(x = age, y = pi4)) + 
  geom_ribbon(aes(ymin = piLower4, ymax = piUpper4, fill = class), alpha = 0.2) +
  geom_line(aes(color = class), lwd = 1) + 
  ylab("Probability of Survival") +
  facet_wrap(vars(sex))
```
