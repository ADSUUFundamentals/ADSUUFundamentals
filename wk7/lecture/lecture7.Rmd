---
title: "More Logistic Regression"
author: "Kyle M. Lang"
date: "Fundamental Techniques in Data Science with R"
output:
  ioslides_presentation:
    smaller: yes
    widescreen: no
   # logo: logo.png
---


## Packages Used

```{r message = FALSE, warning = FALSE}
library(magrittr) # pipes
library(dplyr)    # data manipulation
library(lattice)  # conditional plotting
library(ggplot2)  # plotting
library(DAAG)     # cross-validation
library(caret)    # confusion matrices
```

```{r, echo = FALSE}
source("../../code/supportFunctions.R")

## Make ggplot backgrounds transparent:
theme_update(plot.background = element_rect(fill = "transparent", colour = NA))

knitr::opts_chunk$set(dev.args = list(bg = "transparent"), 
                      dev = "svg",
                      warning = FALSE,
                      message = FALSE)
```


## Recap

With **logistic regression** we can model a binary outcome using a set of 
continuous and/or categorical predictors. 

- If we use **linear regression** to model a binary outcome variable, we would 
create a *linear probability model*.

The linear probability model is a (bad) way to describe the conditional 
probabilities.

- The residual variance is not constant (violation of homoscedasticity)
- The residuals are not normally distributed

Because the linear probability model violates some assumptions of the linear 
model, the standard errors and hypothesis tests are not valid. 

- In short, you may draw invalid conclusions.  


# Titanic data


## Example: Titanic Data

We will begin this lecture with a data set that records the survival of the
passengers on the maiden voyage of the Titanic.

```{r}
titanic <- readRDS("data/titanic.rds")
titanic %>% head()
```


## Inspect the Data

```{r}
str(titanic)
```


## Available Variables

The outcome/dependent variable:

- *survived*: Did the passenger survive?

Some potential predictors:

- *sex*: The passenger's sex 
- *class*: The class of the passenger's ticket
- *age*: The passenger's age in years
- *siblings_spouses*: Number of siblings + spouses traveling with the passenger
- *parents_children*: Number of parents + children traveling with the passenger


## Potential Hypotheses

We can investigating patterns in these data that relate to the survival 
probability. 

Based on the creed "women and children first", we could hypothesize that 

- `age` relates to the probability of survival $\rightarrow$ younger passengers 
have a higher probability of survival
- `sex` relates to the probability of survival $\rightarrow$ females have a 
higher probability of survival

Based on socioeconomic status, we could hypothesize that

- `class` relates to the probability of survival $\rightarrow$ passengers 
traveling with higher classes have a higher probability of survival


# A quick investigation


## Process the Data

Add a numeric version of the outcome:
```{r}
titanic <- mutate(titanic, survived_dummy = as.numeric(survived) - 1)
```

Split the data
```{r}
set.seed(235711)
filter <- c(rep(TRUE, 800), rep(FALSE, nrow(titanic) - 800)) %>% sample()
train  <- titanic[filter, ]
test   <- titanic[!filter, ]
```


## Is `age` related to `survival`?

```{r fig.height = 3}

ggplot(train, aes(x = age, y = survived_dummy)) + 
  geom_point() +
  geom_smooth(method = "glm", 
              method.args = list(family = "binomial"), 
              se = FALSE)
```


## Check surival rates by class

```{r}
train %$% table(class, survived)
```

Higher classes seem to have higher probabilities of survival.

- Converting the counts to marginal proportions will clarify the story.

```{r}
train %$% 
  table(class, survived) %>% 
  prop.table(margin = 1) %>% 
  round(2)
```


# Modeling Surival Probability


## Conditional Distributions of Age

```{r fig.height = 3}
train %$% histogram(~ age | survived)
```

The distribution of `age` for survivors is different from the distribution of 
`age` for non-survivors. 

- There is a point-mass for young survivors $\Rightarrow$ children have higher 
chances of survival. 

## Model Estimates

```{r}
glm(survived ~ age, data = train, family = "binomial") %>% summary()
```


## Conditional Distributions of Sex

```{r fig.height = 3}
train %$% histogram(~ sex | survived)
```

These distributions are very different! 

- Females seem to have a much higher probability of survival. 


## Model Estimates

```{r fig.height = 3}
glm(survived ~ sex, data = train, family = "binomial") %>% summary()
```


## Conditional Distributions of Class

```{r fig.height=3}
train %$% histogram(~ class | survived)
```

There is an obvious difference between the distributions of survivors and 
non-survivors over the classes. 

- In 1st and 2nd class, there are more survivors than non-survivors
- In 3rd class the relation reversed 


## Model Estimates

```{r}
glm(survived ~ class, data = train, family = "binomial") %>% summary()
```


## Multiple Logistic Regression Model

```{r}
fit1 <- glm(survived ~ age + sex + class, data = train, family = "binomial")
partSummary(fit1, -1)
```


## Add Interactions

```{r}
fit2 <- glm(survived ~ age * sex + age * class + sex * class, 
            data = train,
            family = "binomial")
summary(fit2)$coefficients
```


## Model Comparison

Test the change in deviance.
```{r}
anova(fit1, fit2, test = "Chisq") 
```


## Model Comparison

Compare information criteria 
```{r}
AIC(fit1, fit2)
BIC(fit1, fit2)
```


## Cross Validation

```{r}
set.seed(235711)
cv1 <- CVbinary(fit1, nfolds = 10)

set.seed(235711)
cv2 <- CVbinary(fit2, nfolds = 10)

c(fit1 = cv1$acc.cv, fit2 = cv2$acc.cv)
```


# Estimation


## Maximum Likelihood Estimation

To estimate a logistic regression model, we need to use *maximum likelihood* 
(ML) estimation. 

- Maximize the *likelihood* function, $L(Y|\beta)$.
- Equivalent to minimizing the *deviance*, $D = -2L(Y|\beta)$.

Conceptually, ML estimation amounts to the following:

1. Assume a distribution for the outcome data.
1. Pick some candidate values for the parameters of that distribution.
1. Calculate the probability of observing our data if they were really generated 
from a distribution as defined in Steps 1 & 2.
1. Choose "better" parameter values and repeat Step 3.


## ML Intuition

Let's say we have the following $N = 10$ observations.

- We assume these data come from a normal distribution with a known variance of 
$\sigma^2 = 1$.
- We want to estimate the mean of this distribution, $\mu$.

```{r}
(y <- rnorm(n = 10, mean = 5, sd = 1))
```

In ML estimation, we would define different normal distributions.

- Every distribution would have $\sigma^2 = 1$
- Each distribution would have a different value of $\mu$.

We then compare the observed data to those distributions and see which 
distribution best fits the data.


## ML Intuition

```{r, echo = FALSE}
x1 <- seq(0, 10, length.out = 1000)
y1 <- dnorm(x1, 3, 1)
y2 <- dnorm(x1, 4, 1)
y3 <- dnorm(x1, 5, 1)
y4 <- dnorm(x1, 6, 1)

dat1 <- data.frame(m = rep(c(3, 4, 5, 6), each = length(x1)),
                   x = rep(x1, 4), 
                   y = c(y1, y2, y3, y4)
                   )

dat2 <- data.frame(m = rep(c(3, 4, 5, 6), each = length(y)),
                   x = rep(y, 4),
                   y = c(dnorm(y, 3, 1),
                         dnorm(y, 4, 1),
                         dnorm(y, 5, 1),
                         dnorm(y, 6, 1)
                         )
                   )

ggplot(data = dat1, mapping = aes(y = y, x = x)) + 
  geom_line() + 
  geom_segment(data = dat2, mapping = aes(x = x, xend = x, y = 0, yend = y), color = "red") +
  theme_classic() +
  ylab("Density") +
  xlab("Data Values") +
  facet_wrap(vars(m))
```


# Assumptions


## Assumptions of Logistic Regression

We can state the assumptions of linear regression as follows:

1. The outcome follows a binomial distribution.
1. The predictor matrix is full-rank.
1. The predictors are linearly related to $logit(\pi)$.
1. The observations are independent after accounting for the predictors.

Unlike linear regression, we don't need to assume

- Constant, finite error variance
- Normally distributed errors

For computational reasons, we also need the following:

- Large sample
- Relatively well-balance outcome
- No highly influential cases


## Linearity 

Recall the two ways we can visualize our model's predictions.

```{r, echo = FALSE, fig.height = 3}
x   <- rnorm(1000)
eta <- 3 * x
p   <- plogis(eta)

dat1 <- data.frame(type = rep(c("Logit", "Probability"), each = length(x)), 
                   x = rep(x, 2), 
                   y = c(eta, p)
                   )

ggplot(data = dat1, mapping = aes(y = y, x = x)) + 
  geom_point() + 
  xlab("Predictor") +
  ylab("Outcome") +
  facet_wrap(vars(type), scales = "free")
```

The $logit()$ function *linearizes* the success probability.

- The relationship between the (possibly transformed) $X$ variables and 
$logit(\pi)$ should be a straight line.


## Non-Constant Variance

The mean of the binomial distribution is the success probability: $\bar{Y} = \pi$.

The variance is a function of the mean: $\textrm{var}(Y) = \pi (1 - \pi)$.

```{r, echo = FALSE, fig.height = 4}
eta <- predict(fit2, type = "link")
var <- plogis(eta) * (1 - plogis(eta))
dat <- data.frame(eta, var)

ggplot(dat, aes(x = eta, y = var)) + 
  geom_point() +
  ylab("Variance") +
  xlab("Fitted Logit Values")
```


## Residuals in Logistic Regression

There are many ways to define a residual in logistic regression.

- Response residuals
   - $\hat{\varepsilon}_{r,i} = Y_i - \hat{\pi}_i$
   - Direct analogue of residuals in linear regression
   - Not very useful for logistic regression
   
- Pearson residuals
   - $\hat{\varepsilon}_{p,i} = \frac{\hat{\varepsilon}_{r,i}}{\sqrt{\hat{\pi}_i(1 - \hat{\pi}_i)}}$
   - Addresses heterogeneity by dividing out the variance of the $i$th observation
   
- Deviance residuals
   - $\hat{\varepsilon}_{d,i} = \textrm{sign}(\hat{\varepsilon}_{r,i}) \sqrt{d_i}$
   - Most natural type of residual for logistic regression
   - Based on the objective function being optimized to estimate the model
   

## Visualizing Different Residuals

```{r, eval = FALSE}
## Compute different flavors of residual:
rr <- resid(fit2, type = "response")
rp <- resid(fit2, type = "pearson")
rd <- resid(fit2, type = "deviance")

## Compute fitted logit values:
eta <- predict(fit2, type = "link")

## Aggregate and plot the data:
rDat <- data.frame(Residual = c(rr, rp, rd),
                   Eta      = rep(eta, 3),
                   Type     = rep(c("Response", "Pearson", "Deviance"), 
                                  each = length(eta)
                                  )
                   )

ggplot(rDat, aes(x = Eta, y = Residual, color = Type)) + 
  geom_point(alpha = 0.35) +
  geom_smooth(se = FALSE)
```


## Visualizing Different Residuals

```{r, echo = FALSE}
## Compute different flavors of residual:
rr <- resid(fit2, type = "response")
rp <- resid(fit2, type = "pearson")
rd <- resid(fit2, type = "deviance")

## Compute fitted logit values:
eta <- predict(fit2, type = "link")

## Aggregate and plot the data:
rDat <- data.frame(Residual = c(rr, rp, rd),
                   Eta      = rep(eta, 3),
                   Type     = rep(c("Response", "Pearson", "Deviance"), 
                                  each = length(eta)
                                  )
                   )

ggplot(rDat, aes(x = Eta, y = Residual, color = Type)) + 
  geom_point(alpha = 0.35) +
  geom_smooth(se = FALSE)
```


## Diagnostics: Linearity

```{r}
plot(fit2, 1) # Pearson residuals
```


## Diagnostics: Influential Cases

```{r}
plot(fit2, 4)
```


## Diagnostics: Influencial Cases

```{r}
plot(fit2, 5)
```


# Classification


## Generate Predictions on the Logit Scale

```{r}
etaHat <- predict(fit2, newdata = test, type = "link", se.fit = TRUE)
sapply(etaHat, head)
```


## Generate Predicted Probabilities

Calculate $\hat{\pi}$ directly using the `predict()` function:
```{r}
piHat <- predict(fit2, newdata = test, type = "response")
```

Apply the logistic function, `plogis()`, to the $\hat{\eta}$ values we computed 
earlier:
```{r}
piHat2 <- plogis(etaHat$fit)
head(cbind(piHat, piHat2))
```


## Generate Classifications

```{r}
yHat <- ifelse(piHat > 0.5, "yes", "no") %>% factor()
table(yHat)
```

Calculate the confusion matrix

```{r}
cm <- table(pred = yHat, true = test$survived)
cm
```


## Sensitivity, Specificity, & Accuracy
```{r}
(sensitivity <- cm["yes", "yes"] / sum(cm[ , "yes"]))
(specificity <- cm["no", "no"] / sum(cm[ , "no"]))
(acc <- diag(cm) %>% sum() / sum(cm))
```

We can also use the *caret::confusionMatrix()* function:
```{r eval = FALSE}
confusionMatrix(yHat, reference = test$survived)
```


## Output from confusionMatrix() {.smaller}

```{r, echo = FALSE}
confusionMatrix(yHat, reference = test$survived)
```


# Visualization


## Augment the Data

First, we'll add the predicted quantities to the training data.

- We'll need these for plotting.

```{r}
tmp <- predict(fit2, type = "link", se.fit = TRUE)

train$eta <- tmp$fit
train$se  <- tmp$se.fit
train$pi  <- plogis(tmp$fit)
```

Next, we add confidence intervals for the fitted values.

```{r}
train %<>% 
  mutate(etaLower = eta - 1.96 * se, 
         etaUpper = eta + 1.96 * se,
         piLower = plogis(etaLower),
         piUpper = plogis(etaUpper)
         )
```


## Visualizing the Estimates (Logit)

```{r fig.height = 3.5}
ggplot(train, aes(x = age, y = eta)) + 
  geom_line(aes(color = class), lwd = 1) +
  geom_ribbon(aes(ymin = etaLower, ymax = etaUpper, fill = class), alpha = 0.2) +
  ylab("Logit of Survival") +
  facet_wrap(vars(sex))
```


## Visualizing the Estimates (Probability)

```{r fig.height = 3.5}
ggplot(train, aes(x = age, y = pi)) + 
  geom_ribbon(aes(ymin = piLower, ymax = piUpper, fill = class), alpha = 0.2) +
  geom_line(aes(color = class), lwd = 1) + 
  ylab("Probability of Survival") +
  facet_wrap(vars(sex))
```


## Additive Model

Augment the data with fitted values from the additive model:

```{r}
tmp <- predict(fit1, type = "link", se = TRUE)
train$eta <- tmp$fit
train$se  <- tmp$se.fit
train$pi  <- plogis(tmp$fit)

train %<>% 
  mutate(etaLower = eta - 1.96 * se, 
         etaUpper = eta + 1.96 * se,
         piLower = plogis(etaLower),
         piUpper = plogis(etaUpper)
         )
```


## Additive Model (Logit)

```{r fig.height = 3.5}
ggplot(train, aes(x = age, y = eta)) + 
  geom_line(aes(color = class), lwd = 1) +
  geom_ribbon(aes(ymin = etaLower, ymax = etaUpper, fill = class), alpha = 0.2) +
  ylab("Logit of Survival") +
  facet_wrap(vars(sex))
```


## Additive Model (Probability)

```{r fig.height=3.5}
ggplot(train, aes(x = age, y = pi)) + 
  geom_ribbon(aes(ymin = piLower, ymax = piUpper, fill = class), alpha = 0.2) +
  geom_line(aes(color = class), lwd = 1) + 
  ylab("Probability of Survival") +
  facet_wrap(vars(sex))
```
