---
title: "How wrong may a useful model be?"
author: "Gerko Vink"
date: "Fundamental Techniques in Data Science with R"
output:
  ioslides_presentation:
    logo: logo.png
    smaller: yes
    widescreen: no
---

# Today 

## This lecture

- visualizing models with `ggplot2`

- model fit

- model complexity

- cross validation

##  We use the following packages
```{r message=FALSE}
library(MASS)
library(dplyr)
library(magrittr)
library(ggplot2)
library(mice)
library(DAAG)
library(car)
```

# Model fit

##  `anscombe` example {.smaller}
```{r message=FALSE, warning = FALSE}
fit <- anscombe %$%
  lm(y1 ~ x1)

summary(fit)
```

## New data example
```{r cache = TRUE}
boys.fit <- 
  na.omit(boys) %$% # Extremely wasteful
  lm(age ~ reg)
```

```{r}
boys %>% 
  head
```


## Model {.smaller}
```{r cache = TRUE}
boys.fit %>%
  anova()
```

##
```{r cache = TRUE}
boys.fit %>%
  summary()
```

## Model factors
```{r cache = TRUE}
boys.fit %>%
  model.matrix() %>%
  head()
```

## `aov()`
```{r cache = TRUE}
boys.fit %>% 
  aov()
```

`aov()` is for balanced designs.

## `anova()`
```{r cache = TRUE}
boys.fit %>% 
  anova()
```

## `Anova()` with type I Sum of Squares

1. SS(A) for factor A.
2. SS(B | A) for factor B.
3. SS(AB | B, A) for interaction AB.

- If data are unbalanced, the ordering defines the estimates:
  - Different ordering of factors will yield different effects 

## `Anova()` with type II Sum of Squares

1. SS(A | B) for factor A.
2. SS(B | A) for factor B.

No interaction assumed. So test for this first!

- only if interaction is not significant continue with interpretation of the main effects. 
- most powerful if there is no interaction

## `Anova()` with type II Sum of Squares
```{r cache = TRUE}
boys.fit %>% 
  car::Anova(type = 2)
```

## `Anova()` with type III Sum of Squares

1. SS(A | B, AB) for factor A.
2. SS(B | A, AB) for factor B.

Tests if there is a main effect, given the other main effect and the interaction. 

- however, most often we do not care about interpretation of the main effects if the interaction is significant

`When data are balanced, Type I, II and II are identical, because the factors are orthogonal`

## `Anova()` with type III Sum of Squares
```{r cache = TRUE}
boys.fit %>% 
  car::Anova(type = 3)
```

## Post hoc comparisons
```{r cache = TRUE}
coef <- 
  boys.fit %>%
  aov() %>%
  summary.lm()
coef
```

## Post hoc comparisons
```{r cache = TRUE}
p.val <- coef$coefficients
p.adjust(p.val[, "Pr(>|t|)"], method = "bonferroni")
```

## AIC and BIC
Akaike's *An Information Criterion* 
```{r cache = TRUE}
boys.fit %>% 
  AIC()
```

and *Bayesian Information Criterion*
```{r cache = TRUE}
boys.fit %>%
  BIC()
```

## Model comparison
```{r cache = TRUE}
boys.fit2 <- 
  na.omit(boys) %$%
  lm(age ~ reg + hgt)

boys.fit %>% AIC()
boys.fit2 %>% AIC()
```

## Another model
```{r cache = TRUE}
boys.fit3 <- 
  na.omit(boys) %$%
  lm(age ~ reg + hgt * wgt)
```
is equivalent to 
```{r eval=FALSE}
boys.fit3 <- 
  na.omit(boys) %$%
  lm(age ~ reg + hgt + wgt + hgt:wgt)
```

## Model comparison
```{r cache = TRUE}
boys.fit %>% AIC()
boys.fit2 %>% AIC()
boys.fit3 %>% AIC()
```

## Model comparison
```{r cache = TRUE}
anova(boys.fit, boys.fit2, boys.fit3)
```

## Model fit again
```{r cache = TRUE}
boys.fit3 %>%
  car::Anova(type = 3)
```

## Model fit again
```{r cache = TRUE}
boys.fit3 %>%
  car::Anova(type = 2)
```

## Model fit again
```{r cache = TRUE}
boys.fit3 %>%
  anova()
```

## Influence of cases
DfBeta calculates the change in coefficients depicted as deviation in SE's.
```{r cache = TRUE}
boys.fit3 %>%
  dfbeta() %>%
  head(n = 7)
```

# Prediction

## Fitted values:
Let's use the simpler `anscombe` data example
```{r cache = TRUE}
y_hat <- 
  fit %>%
  fitted.values()
```
The residual is then calculated as
```{r cache = TRUE}
y_hat - anscombe$y1
```

## Predict new values
If we introduce new values for the predictor `x1`, we can generate predicted values from the model
```{r cache = TRUE, warning=FALSE}
new.x1 <- data.frame(x1 = 1:20)
predict.lm(fit, newdata = new.x1)
```

## Predictions are draws from the regression line
```{r}
pred <- predict.lm(fit, newdata = new.x1)
lm(pred ~ new.x1$x1)$coefficients
fit$coefficients
```

## Prediction intervals
```{r warning=FALSE}
predict(fit, interval = "prediction")
```

# Assessing predictive accuracy

## K-fold cross-validation

- Divide sample in $k$ mutually exclusive training sets
- Do for all $j\in\{1,\dots,k\}$ training sets
  
    1. fit model to training set $j$
    2. obtain predictions for test set $j$  (remaining cases)
    3. compute residual variance (MSE) for test set $j$
  
- Compare MSE in cross-validation with MSE in sample
- Small difference suggests good predictive accuracy

## K-fold cross-validation `anscombe` data {.smaller}

  -  residual variance sample is $1.24^2 \approx 1.53$
  -  residual variance cross-validation is 1.93 
  -  regression lines in the 3 folds are similar

```{r echo=F, fig.height=4, dev.args = list(bg = 'transparent')}
par(mar=c(5,4,4,2)+.1,cex=.8,cex.lab=1.5,cex.axis=1.2,cex.main=1.5)
```
```{r echo=T, fig.height=3.5, dev.args = list(bg = 'transparent')}
DAAG::CVlm(anscombe, fit, plotit=T, printit=F)
```

## K-fold cross-validation `boys` data {.smaller}

  -  residual variance sample is 2
  -  residual variance cross-validation is 2.38
  -  regression lines in the 3 folds almost identical

```{r echo=F, fig.height=4, dev.args = list(bg = 'transparent')}
par(mar=c(5,4,4,2)+.1,cex=.8,cex.lab=1.5,cex.axis=1.2,cex.main=1.5)
```
```{r echo=T, fig.height=3.5, dev.args = list(bg = 'transparent'), warning=FALSE}
DAAG::CVlm(na.omit(boys), boys.fit3, plotit=T, printit=F)
```

## How many cases are used?
```{r}
na.omit(boys) %$%
  lm(age ~ reg + hgt * wgt) %>%
  nobs()

```

If we would not have used `na.omit()`
```{r}
boys %$%
  lm(age ~ reg + hgt * wgt) %>%
  nobs()
```

## Some other modeling devices in `R`
- `lm()`: linear modeling
- `glm()`: generalized linear modeling
- `gamlss::gamlss`: generalized additive models (for location, scale and shape)
- `lme4::lmer`: linear mixed effect models
- `lme4::glmer`: generalized linear mixed effect models
- `lme4::nlmer`: non-linear mixed effect models

