---
title: "Pipes and least squares estimation"
author: "Gerko Vink"
date: "Fundamental Techniques in Data Science with R"
output:
  ioslides_presentation:
    logo: logo.png
    smaller: yes
    widescreen: no
---

# Today 

## This lecture

- Leftover slides from last week

- Data manipulation

- Basic analysis (correlation & t-test)

- Pipes

- Deviations and modeling

## Some programming tips:
- keep your code tidy
- use comments (text preceded by `#`) to clarify what you are doing
    - If you look at your code again, one month from now: you will not know what you did --> unless you use comments 

- when working with functions, use the TAB key to quickly access the help for the function's components
- work with logically named `R`-scripts
    - indicate the sequential nature of your work
- work with `RStudio` projects
- if allowed, place your project folders in some cloud-based environment

## Functions

Functions have parentheses `()`. Names directly followed by parentheses always indicate functions. For example; 

 - `matrix()` is a function
 - `c()` is a function
 - but `(1 - 2) * 5` is a calculation, not a function

## Packages
Packages give additional functionality to `R`. 

By default, some packages are included. These packages allow you to do mainstream statistical analyses and data manipulation. Installing additional packages allow you to perform the state of the art in statistical programming and estimation. 

The cool thing is that these packages are all developed by users. The throughput process is therefore very timely:

  - newly developed functions and software are readily available
  - this is different from other mainstream software, like SPSS, where new methodology may take years to be implemented. 

A list of available packages can be found on [CRAN](https://cran.r-project.org)

## Loading packages

There are two ways to load a package in `R`
```{r}
library(stats)
``` 
and
```{r}
require(stats)
```

## Installing packages
The easiest way to install e.g. package `mice` is to use
```{r eval=FALSE}
install.packages("mice")
```

Alternatively, you can also do it in `RStudio` through 

`Tools --> Install Packages`

# `R` in depth

## Workspaces and why you should sometimes save them
A workspace contains all changes you made to `R`. 

A saved workspace contains everything at the time of the state wherein it was saved. 

You do not need to run all the previous code again if you would like to continue working at a later time. 

- You can save the workspace and continue exactly where you left. 

Workspaces are compressed and require relatively little memory when stored. The compression is very efficient and beats reloading large datasets. 

## History and why it is useful
`R` by default saves (part of) the code history and `RStudio` expands this functionality greatly. 

Most often it may be useful to look back at the code history for various reasons.

- There are multiple ways to access the code history.
  
    1. Use arrow up in the console. This allows you to go back in time, one codeline by one. Extremely useful to go back to previous lines for minor alterations to the code.
    2. Use the history tab in the environment pane. The complete project history can be found here and the history can be searched. This is particularly convenient when you know what code you are looking for. 
    
## Working in projects in `RStudio`
- Every project has its own history
- Every research project has its own project
- Every project can have its own folder, which also serves as a research archive
- Every project can have its own version control system
- R-studio projects can relate to Git (or other online) repositories

## In general...
- Use common sense and BE CONSISTENT.

- Browse through [the tidyverse style guide](https://style.tidyverse.org)

  - The point of having style guidelines is to have a common vocabulary of coding 
  - so people can concentrate on what you are saying, rather than on how you are saying it. 

- If code you add to a file looks drastically different from the existing code around it, the discontinuity will throw readers and collaborators out of their rhythm when they go to read it. Try to avoid this.

- Intentional spacing makes your code easier to interpret

  - `a<-c(1,2,3,4,5)` vs;
  - `a <- c(1, 2, 3, 4, 5)`
 
 - at least put a space after every comma!

## New packages we use
```{r warning=FALSE, message=FALSE}
library(MASS)     # for the cats data
library(dplyr)    # data manipulation
library(haven)    # in/exporting data
library(magrittr) # pipes
library(mice)     # for the nhanes data
```

<img src="pipe.jpg" style="display:block;width:500px;margin-left:auto;margin-right:auto"></img>

## New functions

- `transform()`: changing and adding columns
- `dplyr::filter()`: row-wise selection (of cases)
- `table()`: frequency tables
- `class()`: object class
- `levels()`: levels of a factor
- `order()`: data entries in increasing order
- `haven::read_sav()`: import SPSS data
- `cor()`: bivariate correlation
- `sample()`: drawing a sample
- `t.test()`: t-test 

# Data manipulation

## The cats data
```{r}
head(cats)
```


```{r}
str(cats)
```

## How to get only Female cats?

```{r}
fem.cats <- cats[cats$Sex == "F", ]
dim(fem.cats)
head(fem.cats)
```

## How to get only *heavy* cats?
```{r}
heavy.cats <- cats[cats$Bwt > 3, ]
dim(heavy.cats)
head(heavy.cats)
```

## How to get only *heavy* cats?
```{r}
heavy.cats <- subset(cats, Bwt > 3)
dim(heavy.cats)
head(heavy.cats)
```

## another way: `dplyr`
```{r}
filter(cats, Bwt > 2, Bwt < 2.2, Sex == "F")
```


## Working with factors
```{r}
class(cats$Sex)
levels(cats$Sex)
```

## Working with factors
```{r}
levels(cats$Sex) <- c("Female", "Male")
table(cats$Sex)
head(cats)
```

## Sorting 

```{r}
sorted.cats <- cats[order(cats$Bwt), ]
head(sorted.cats)
```


## Combining matrices or dataframes

```{r}
cats.numbers <- cbind(Weight = cats$Bwt, HeartWeight = cats$Hwt)
head(cats.numbers)
```

## Combining matrices or dataframes
```{r}
rbind(cats[1:3, ], cats[1:5, ])
```


# Basic analysis
## Correlation

```{r}
cor(cats[, -1])
```
With `[, -1]` we exclude the first column

## Correlation

```{r}
cor.test(cats$Bwt, cats$Hwt)
```

What do we conclude?

## Correlation

```{r fig.height=5, fig.width=5, dev.args = list(bg = 'transparent'), fig.align='center'}
plot(cats$Bwt, cats$Hwt)
```

## T-test
Test the null hypothesis that the difference in mean heart weight between male and female cats is 0
```{r}
t.test(formula = Hwt ~ Sex, data = cats)
```

## T-test
```{r fig.height=5, fig.width=5, dev.args = list(bg = 'transparent'), fig.align='center'}
plot(formula = Hwt ~ Sex, data = cats)
```

# Pipes

## This is a pipe:

```{r message=FALSE, eval = FALSE}
boys <- 
  read_sav("boys.sav") %>%
  head()
```

It effectively replaces `head(read_sav("boys.sav"))`.


## Why are pipes useful?
Let's assume that we want to load data, change a variable, filter cases and select columns. Without a pipe, this would look like
```{r}
boys  <- read_sav("boys.sav")
boys2 <- transform(boys, hgt = hgt / 100)
boys3 <- filter(boys2, age > 15)
boys4 <- subset(boys3, select = c(hgt, wgt, bmi))
```

With the pipe:
```{r}
boys <-
  read_sav("boys.sav") %>%
  transform(hgt = hgt/100) %>%
  filter(age > 15) %>%
  subset(select = c(hgt, wgt, bmi))
```

Benefit: a single object in memory that is easy to interpret


## With pipes
Your code becomes more readable:

- data operations are structured from left-to-right and not from in-to-out
- nested function calls are avoided
- local variables and copied objects are avoided
- easy to add steps in the sequence

## What do pipes do:

- `f(x)` becomes `x %>% f()`
```{r}
rnorm(10) %>% mean()
```
- `f(x, y)` becomes `x %>% f(y)` 
```{r}
boys %>% cor(use = "pairwise.complete.obs")
```
- `h(g(f(x)))` becomes `x %>% f %>% g %>% h` 
```{r}
boys %>% subset(select = wgt) %>% na.omit() %>% max()
```

## Useful: outlier filtering
```{r}
nrow(cats)

cats.outl <- 
  cats %>% 
  filter(Hwt < mean(Hwt) + 3 * sd(Hwt), 
         Hwt > mean(Hwt) - 3 * sd(Hwt))

nrow(cats.outl)

cats %>%  
  filter(Hwt > mean(Hwt) + 3 * sd(Hwt))
```

# More pipe stuff

## The standard `%>%` pipe
<center>
<img src="flow_pipe.png" alt="HTML5 Icon" width = 75%>
</center>

## The `%$%` pipe
<center>
<img src="flow_$_pipe.png" alt="HTML5 Icon" width = 75%>
</center>

## The role of `.` in a pipe
In `a %>% b(arg1, arg2, arg3)`, `a` will become `arg1`. With `.` we can change this.
```{r error=TRUE}
cats %>%
  plot(Hwt ~ Bwt, data = .)
```
VS
```{r}
cats %$%
  plot(Hwt ~ Bwt)
```
The `.` can be used as a placeholder in the pipe. 

## Performing a t-test in a pipe
```{r message=FALSE}
cats %$%
  t.test(Hwt ~ Sex)
```
is the same as 
```{r eval=FALSE}
t.test(Hwt ~ Sex, data = cats)
```

## Storing a t-test from a pipe
```{r}
cats.test <- cats %$%
  t.test(Bwt ~ Sex)

cats.test
```

# Squared deviations

## We have already met deviations today

- correlations

$$\rho_{X,Y} = \frac{\mathrm{cov}(X,Y)}{\sigma_X\sigma_Y} = \frac{\mathrm{E}[(X - \mu_X)(Y-\mu_Y)]}{\sigma_X\sigma_Y}.$$

- t-test

$$t = \frac{\bar{X}-\mu}{\hat{\sigma}/\sqrt{n}}.$$

- variance

$$\sigma^2_X = \mathrm{E}[(X - \mu)^2].$$

Can you identify which part of these equations are the deviations?

## Deviations

Deviations tell what the distance is for each value (`observation`) to a comparison value. 

 - often, the mean is chosen as a comparison value.
 
### Why the mean?

The arithmetic mean is a very informative measure:

- it is the average
- it is the mathematical expectation
- it is the central value of a set of discrete numbers

$$ $$
\[\text{The mean itself is a model: observations are}\]
\[\text{merely  a deviation from that model}\]

## The mean as a center
```{r echo=FALSE}
library(ggplot2)
set.seed(123)
plotdata <- data.frame(X = rnorm(100, 167.5, 10),
           Y = rnorm(100, 180.8, 10)) 
plotdata %>%
  ggplot(aes(X, Y)) + 
  geom_point(color = "blue") + 
  geom_point(aes(mean(X), mean(Y)), color = "orange", size = 5) + 
  geom_vline(xintercept = 168.4041, color = "orange") + 
  geom_hline(yintercept = 179.7245 , color = "orange") + 
  ggtitle(bquote("Bivariate normal")) + 
  theme_minimal()
```

## Deviations from the mean
```{r echo=FALSE}
plotdata %>%
  ggplot(aes(X, Y)) + 
  geom_point(color = "blue") + 
  geom_point(aes(mean(X), mean(Y)), color = "orange", size = 5) + 
  geom_segment(aes(xend = 168.4041, yend = Y), color = "orange", lty = 2, alpha = .5) +
  geom_vline(xintercept = 168.4041, color = "orange") + 
  ggtitle(bquote("Univariate" ~X )) + 
  theme_minimal()
```

## Plotting the deviations
```{r echo=FALSE}
plotdata %>%
  mutate(deviation = X-mean(X)) %>%
  ggplot(aes(deviation)) + 
  geom_density(color = "blue") + 
  geom_vline(xintercept = 0, color = "orange") + 
  ggtitle(expression(X - bar(X))) + xlab("Deviation from the mean X") + 
  theme_minimal()
```

## Use of deviations
Deviations summarize the fit of all the points in the data to a single point

The mean is the mathematical expectation. It represents the observed values best for a normally distributed univariate set.
 
 - The mean yields the lowest set of deviations
```{r}
plotdata %>%
  mutate("Mean" = X - mean(X), 
         "Mean + 3" = X - (mean(X) + 3)) %>%
  select("Mean", "Mean + 3") %>%
  colSums %>%
  round(digits = 3)
```

The mean minimizes the deviations

## What happens
```{r echo=FALSE}
plotdata %>%
  mutate(deviation = X - mean(X), 
         deviation2 = X - (mean(X) + 3)) %>%
  ggplot() + 
  geom_density(aes(deviation), color = "blue") + 
  geom_density(aes(deviation2), color = "orange") + 
  geom_vline(xintercept = -3, color = "orange") +  
  geom_vline(xintercept = 0, color = "blue") +  
  ggtitle(expression(paste("Deviations from ", bar(X), " (blue) and ", bar(X) + 3, " (orange)")))+
  theme_minimal()
```

## Plotting the standardized deviations
```{r echo=FALSE}
plotdata %>%
  mutate(deviation = (X-mean(X))/sd(X)) %>%
  ggplot(aes(deviation)) + 
  geom_density(color = "blue") + 
  geom_vline(xintercept = 0, color = "orange") + 
  ggtitle(expression((X - bar(X)) / sigma(X))) + xlab("Standardized deviation from the mean X") + 
  theme_minimal()
```

## Plotting the squared deviations
```{r echo=FALSE}
plotdata %>%
  mutate(deviation = (X-mean(X))^2) %>%
  ggplot(aes(deviation)) + 
  geom_density(color = "blue") + 
  geom_vline(xintercept = 0, color = "orange") + 
  ggtitle(expression((X - bar(X))^2)) + xlab("Squared deviation from the mean X") + 
  theme_minimal()
```

## Why squared deviations are useful

Throughout statistics we make extensive use of squaring. 
$$ $$
\[\text{WHAT ARE THE USEFUL PROPERTIES OF SQUARING}\]
\[\text{THAT STATISTICIANS ARE SO FOND OF?}\]

## Deviations from the mean
```{r echo=FALSE}
plotdata %>%
  ggplot(aes(X, Y)) + 
  geom_point(color = "blue") + 
  geom_point(aes(mean(X), mean(Y)), color = "orange", size = 5) +   
  geom_segment(aes(xend = X, yend = 179.7245), color = "orange", lty = 2, alpha = .5) + 
  geom_hline(yintercept = 179.7245, color = "orange") +
  ggtitle(bquote("Univariate" ~Y ))
```

## Deviations from the mean
```{r echo=FALSE}
plotdata %>%
  ggplot(aes(X, Y)) + 
  geom_point(color = "blue") + 
  geom_point(aes(mean(X), mean(Y)), color = "orange", size = 5) + 
  geom_segment(aes(x = X, y = Y, xend = mean(X), yend = mean(Y)), color = "orange", lty = 2, alpha = .5) +
  ggtitle("Multivariate (X, Y)") + 
  theme_minimal()
```

## Least squares solution
```{r echo=FALSE}
fit <- plotdata %$%
  lm(Y~X)

plotdata %>%
  mutate(predicted = predict(fit),
          residuals = residuals(fit)) %>%
  ggplot(aes(X, Y)) +
  geom_smooth(method = "lm", se = FALSE, color = "orange") +
  geom_segment(aes(xend = X, yend = predicted), color = "orange", alpha = .5) +
  geom_point(aes(color = abs(residuals))) + 
  scale_color_continuous(low = "blue", high = "red") +  
  guides(color = FALSE) +  
  geom_point(aes(y = predicted), shape = 1, color = "orange") +
  ggtitle("Multivariate (X, Y)") + 
  theme_minimal()
```

## Least squares solution
```{r echo=FALSE}
plotdata2 <- data.frame(X = rnorm(100, 167.5, 10)) %>%
  mutate(Y = rnorm(100, 0, 10) - X + 335)

fit <- plotdata2 %$%
  lm(Y~X)

 plotdata2 %>%
   mutate(predicted = predict(fit),
          residuals = residuals(fit)) %>%
  ggplot(aes(X, Y)) +
  geom_smooth(method = "lm", se = FALSE, color = "orange") +
  geom_segment(aes(xend = X, yend = predicted), color = "orange", alpha = .5) +
  geom_point(aes(color = abs(residuals))) + 
  scale_color_continuous(low = "blue", high = "red") +  
  guides(color = FALSE) +  
  geom_point(aes(y = predicted), shape = 1, color = "orange") +
  ggtitle("Multivariate (X, Y), higher correlation") +
  theme_minimal()
```

## What's next

During the practical exercises this week we will learn to calculate and explore squared deviations. During these exercises you'll learn to minimize the deviations. 

Next week we'll jump to regression. We'll see how deviations still play a role in that framework:

  - We minimize deviations to infer a linear model
  - If our model is not perfect, observations will still deviate from the model

