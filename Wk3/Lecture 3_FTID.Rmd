---
title: "Linear modeling and its assumptions"
author: "Gerko Vink"
date: "Fundamental Techniques in Data Science with R"
output:
  ioslides_presentation:
    logo: logo.png
    smaller: yes
    widescreen: no
---

## Last week

A review of last week's contents

## Today
- the linear model
- assumptions associated with the linear model

##  We use the following packages
```{r message=FALSE}
library(MASS)
library(dplyr)
library(magrittr)
library(ggplot2)
library(mice)
library(DAAG)
library(car)
```

# Statistical models

## Statistical model

The mathematical formulation of relationship between variables can be written as

\[
\mbox{observed}=\mbox{predicted}+\mbox{error}
\]

or (for the greek people) in notation as
\[y=\mu+\varepsilon\]

where

-   $\mu$ (mean) is the part of $Y$ that is explained by model 
-  $\varepsilon$ (residual) is the part of $Y$ that is not explained by model 


## A simple example
Regression model:

-  Model individual age from weight

\[
\text{age}_i=\alpha+\beta\cdot{\text{weight}}_i+\varepsilon_i
\]

where

-  $\alpha+\beta{x}_i$ is the mean of `age`, $y$, conditional on `weight`, $x$.
-  $\varepsilon_i$ is random variation 

## The linear model

The function `lm()` is a base function in `R` and allows you to pose a variety of linear models. 

```{r}
args(lm)
```

If we want to know what these arguments do we can ask R:

```{r, eval=FALSE}
?lm
```

This will open a help page on the `lm()` function.

## Continuous predictors {.smaller}
To obtain a linear model with a main effects for `wgt`, we formulate $age\sim wgt$ 
```{r warning=FALSE, message = FALSE}
library(mice)
fit <- with(boys, lm(age ~ wgt))
fit
```

## Continuous predictors: more detail {.smaller}
```{r}
summary(fit)
```

## Continuous predictors
To obtain a linear model with just main effects for `wgt` and `hgt`, we formulate $age\sim wgt + hgt$ 
```{r}
require(mice)
fit <- with(boys, lm(age ~ wgt + hgt))
fit
```

## Continuous predictors: more detail {.smaller}
```{r}
summary(fit)
```

## Continuous predictors: interaction effects
To predict $age$ from $wgt$, $hgt$ and the interaction $wgt*hgt$ we formulate $age\sim wgt * hgt$
```{r}
fit <- with(boys, lm(age ~ wgt * hgt))
fit
```

##  and with more detail {.smaller}
```{r}
summary(fit)
```

## Categorical predictors in the linear model
If a categorical variable is entered into function `lm()`, it is automatically converted to a dummy set in `R`. The first level is always taken as the reference category. If we want another reference category we can use the function `relevel()` to change this.

```{r}
fit <- with(boys, lm(age ~ reg))
```

##  and again with more detail {.smaller}
```{r}
summary(fit)
```

## Components of the linear model {.smaller}
```{r}
names(fit) # the names of the list with output objects
fit$coef  # show the estimated coefficients
coef(fit) # alternative
```


# Fitting a line to data

##  Linear regression
Linear regression model
\[
y_i=\alpha+\beta{x}_i+\varepsilon_i
\]

Assumptions:

  -  $y_i$ conditionally normal with mean $\mu_i=\alpha+\beta{x}_i$
  -  $\varepsilon_i$ are $i.i.d.$ with mean 0 and (constant) variance $\sigma^2$

## The `anscombe` data
```{r}
head(anscombe)
```

Or, alternatively, 
```{r}
anscombe %>%
  head
```

## Fitting a line {.smaller}

```{r eval = FALSE, message = FALSE}
anscombe %>%
  ggplot(aes(y1, x1)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```

##  Fitting a line {.smaller}

```{r echo=FALSE, message = FALSE}
anscombe %>%
  ggplot(aes(y1, x1)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```

##  Fitting a line {.smaller}
The linear model would take the following form:
```{r eval=FALSE}
fit <- 
  yourdata %>%
  lm(youroutcome ~ yourpredictors)

summary(fit)
```
Output:

-  Residuals: minimum, maximum and quartiles
-  Coefficients: estimates, SE's, t-values and $p$-values
-  Fit measures
    -  Residuals SE (standard error residuals)
    -  Multiple R-squared (proportion variance explained)
    -  F-statistic and $p$-value (significance test model)

##  `anscombe` example {.smaller}
```{r message=FALSE, warning = FALSE}
fit <- anscombe %$%
  lm(y1 ~ x1)

summary(fit)
```

## Checking assumptions

1. linearity
    - scatterplot $y$ and $x$ 
    - include loess curve when in doubt
    - does a squared term improve fit?
2. normality residuals
    -  normal probability plots `qqnorm()`
    -  if sample is small; `qqnorm` with simulated errors cf. `rnorm(n, 0, s)` 
3. constant error variance 
    -  residual plot
    -  scale-location plot

## Linearity {.smaller}
```{r eval = FALSE, message = FALSE}
anscombe %>%
  ggplot(aes(x1, y1)) + 
  geom_point() + 
  geom_smooth(method = "loess", col = "blue") + 
  geom_smooth(method = "lm", col = "orange")
```

## Linearity {.smaller}
```{r echo=FALSE, message = FALSE}
anscombe %>%
  ggplot(aes(x1, y1)) + 
  geom_point() + 
  geom_smooth(method = "loess", col = "blue") + 
  geom_smooth(method = "lm", col = "orange")
```

The loess curve suggests slight non-linearity

## Adding a squared term
```{r message=FALSE, warning = FALSE}
anscombe %$%
  lm(y1 ~ x1 + I(x1^2)) %>%
  summary()
```

## Constant error variance? {.smaller}
```{r, fig.height=4, dev.args = list(bg = 'transparent')}
par(mfrow = c(1, 2))
fit %>%
  plot(which = c(1, 3), cex = .6)
```

## No constant error variance! {.smaller}
```{r, fig.height=4, dev.args = list(bg = 'transparent')}
par(mfrow = c(1, 2))
boys %$%
  lm(bmi ~ age) %>%
  plot(which = c(1, 3), cex = .6)
```

## Normality of errors {.smaller}
```{r, fig.height=4, dev.args = list(bg = 'transparent')}
fit %>%
  plot(which = 2, cex = .6)
```

The QQplot shows some divergence from normality at the tails

# Outliers, influence and robust regression

## Outliers and influential cases {.smaller}

Leverage: see the fit line as a lever. 
  
  - some points pull/push harder; they have more leverage
  
Standardized residuals:

  - The values that have more leverage tend to be closer to the line
  - The line is fit so as to be closer to them
  - The residual standard deviation can differ at different points on $X$ - even if the error standard deviation is constant. 
  - Therefore we standardize the residuals so that they have constant variance (assuming homoscedasticity). 

Cook's distance: how far the predicted values would move if your model were fit without the data point in question. 

  - it is a function of the leverage and standardized residual  associated with each data point

```{r echo = FALSE}
set.seed(20)

pred1 = rnorm(20, mean=20, sd=3)
outcome1 = 5 + .5*pred1 + rnorm(20)

pred2 = c(pred1, 30);        outcome2 = c(outcome1, 20.8)
pred3 = c(pred1, 19.44);     outcome3 = c(outcome1, 20.8)
pred4 = c(pred1, 30);        outcome4 = c(outcome1, 10)
```

## Fine
```{r echo=FALSE}
par(mfrow = c(1,2))
plot(outcome1 ~ pred1, ylim = c(9, 25), xlim = c(10, 30))
abline(lm(outcome1 ~ pred1))
plot(lm(outcome1 ~ pred1), which = 5)
```

## High leverage, low residual
```{r echo=FALSE}
par(mfrow = c(1,2))
plot(outcome2 ~ pred2, ylim = c(9, 25), xlim = c(10, 30))
points(30, 20.8, col = "red")
abline(lm(outcome2 ~ pred2))
plot(lm(outcome2 ~ pred2), which = 5)
```

## Low leverage, high residual
```{r echo=FALSE}
par(mfrow = c(1,2))
plot(outcome3 ~ pred3, ylim = c(9, 25), xlim = c(10, 30))
abline(lm(outcome3 ~ pred3))
points(19.44, 20.8, col = "red")
plot(lm(outcome3 ~ pred3), which = 5)
```

## High leverage, high residual
```{r echo=FALSE}
par(mfrow = c(1,2))
plot(outcome4 ~ pred4, ylim = c(9, 25), xlim = c(10, 30))
points(30, 10, col = "red")
abline(lm(outcome4 ~ pred4))
plot(lm(outcome4 ~ pred4), which = 5)
```

## Outliers and influential cases
Outliers are cases with large $e_z$  (standardized residuals).

If the model is ***correct***  we expect:
  
  -  5\% of $|e_z|>1.96$ 
  -  1\% of $|e_z|>2.58$
  -  0\% of $|e_z|>3.3$ 

Influential cases are cases with large influence on parameter estimates
  
  -  cases with Cook's Distance $> 1$, or 
  -  cases with Cook's Distance much larger than the rest

## Outliers and influential cases
```{r, fig.height= 3, dev.args = list(bg = 'transparent')}
par(mfrow = c(1, 2), cex = .6)
fit %>% plot(which = c(4, 5))
```

There are no cases with $|e_z|>2$, so no outliers (right plot). There are no cases with Cook's Distance $>1$, but case 3 stands out 

